{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOdK88Rx4HYwt3SCNyBmB5u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YAMAZAKIAkiraRK/GCAnet/blob/main/DataAugTest4_contrast.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FVcPZTr3T0s",
        "outputId": "dafc5001-91c5-4273-9a41-4c40b81a48a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Colab Notebooks/GCA/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSMoqyJtPIqh",
        "outputId": "0ad18423-d443-4895-86a9-920cb0360a81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/GCA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eTP13Gj3D5m"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.layers import Dense, GlobalAveragePooling2D\n",
        "from keras.models import Model\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import sklearn.preprocessing as sp\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "import preprocess as pp\n",
        "import Grad_Cam as gc\n",
        "from PIL import ImageOps, Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#訓練データの読み込み\n",
        "p_dir = \"YU/grating image/202207/GC2022\"\n",
        "c_dir = os.listdir(p_dir)\n",
        "i = 0\n",
        "data=[]\n",
        "label=[]\n",
        "label_name = []\n",
        "\n",
        "for d in c_dir:\n",
        "    label_name.append(d)\n",
        "    d = os.path.join(p_dir, d)\n",
        "    pp.append_data(d, data, label, i)\n",
        "    i += 1\n",
        "label = np.array(label).reshape(-1,1)\n",
        "#ラベルをone_hot_encoding\n",
        "enc = sp.OneHotEncoder(categories=\"auto\", sparse_output=False, dtype=np.float32)\n",
        "label_enc = enc.fit_transform(label)\n",
        "\n",
        "\n",
        "#テストデータの読み込み\n",
        "p_dir = \"YU/grating image/202207/test data\"\n",
        "c_dir = os.listdir(p_dir)\n",
        "i = 0\n",
        "test_data=[]\n",
        "test_label=[]\n",
        "test_label_name = []\n",
        "for d in c_dir:\n",
        "    test_label_name.append(d)\n",
        "    d = os.path.join(p_dir, d)\n",
        "    pp.append_data(d, test_data, test_label, i)\n",
        "    i += 1"
      ],
      "metadata": {
        "id": "sFrBZFSj3Tzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#前処理-アスペクト比は気にせず299,299に縮小\n",
        "#訓練データ\n",
        "shrink_data = np.empty((len(data),299,299,3))\n",
        "for i in range(len(data)):\n",
        "    shrink_data[i]=cv2.resize(data[i], dsize=(299, 299))\n",
        "shrink_data = np.array(shrink_data)\n",
        "\n",
        "#テストデータ\n",
        "shrink_test_data = np.empty((len(test_data),299,299,3))\n",
        "for i in range(len(test_data)):\n",
        "    shrink_test_data[i]=cv2.resize(test_data[i], dsize=(299, 299))\n",
        "shrink_test_data = np.array(shrink_test_data)"
      ],
      "metadata": {
        "id": "o1xpn__xBjSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#各種autocontrastでデータを複製\n",
        "ac_datas = []\n",
        "for i in range(4):\n",
        "  ac_data = []\n",
        "  for r in range(len(shrink_data)):\n",
        "    ac = shrink_data[r].astype(np.uint8)\n",
        "    ac = Image.fromarray(ac)\n",
        "    cutoff = i\n",
        "    autocon = ImageOps.autocontrast(ac, cutoff)\n",
        "    ac_data.append(np.array(autocon).astype(float))\n",
        "  ac_datas.append(np.array(ac_data))\n",
        "\n",
        "#テストデータも\n",
        "ac_tests = []\n",
        "for i in range(4):\n",
        "  ac_test = []\n",
        "  for r in range(len(shrink_test_data)):\n",
        "    ac = shrink_test_data[r].astype(np.uint8)\n",
        "    ac = Image.fromarray(ac)\n",
        "    cutoff = i\n",
        "    autocon = ImageOps.autocontrast(ac, cutoff)\n",
        "    ac_test.append(np.array(autocon).astype(float))\n",
        "  ac_tests.append(np.array(ac_test))"
      ],
      "metadata": {
        "id": "61tlp3lld-lP"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ja3Ua72S5bPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ac_data = []\n",
        "for r in range(len(shrink_data)):\n",
        "    ac = shrink_data[r].astype(float)\n",
        "    ac = ac-30\n",
        "    ac[ac<0]=0\n",
        "    ac = ac*254/225\n",
        "    ac_data.append(np.array(ac))\n",
        "ac_datas.append(np.array(ac_data))"
      ],
      "metadata": {
        "id": "MUf4HbyU1e7C"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ac_test = []\n",
        "for r in range(len(shrink_test_data)):\n",
        "  ac = shrink_test_data[r].astype(float)\n",
        "  ac = ac-30\n",
        "  ac[ac<0]=0\n",
        "  ac = ac*254/225\n",
        "#  ac = ac.astype(np.uint8)\n",
        "  ac_test.append(np.array(ac))\n",
        "ac_tests.append(np.array(ac_test))"
      ],
      "metadata": {
        "id": "YsPAon3y4qKc"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ac_tests[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKL_Llq9uLiv",
        "outputId": "22cf082d-9ace-4bdf-844f-49c0d5b4e6d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(41, 299, 299, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#オートコントラストのパラメータ別\n",
        "def ACcomparison(datas, label, test_data, test_label):\n",
        "    models=[]\n",
        "    histories=[]\n",
        "    preds=[]\n",
        "    for data in datas:\n",
        "        #GoogLeNetを読み込み、出力層の数を6に変更\n",
        "        base_model = InceptionV3(weights=\"imagenet\", input_shape=(299, 299,3) ,include_top=False)\n",
        "        x = base_model.output\n",
        "        x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "        predictions = Dense(6, activation=\"softmax\")(x)\n",
        "        model = Model(inputs=base_model.input, outputs=predictions)\n",
        "        #モデルのコンパイル\n",
        "        for layer in base_model.layers:\n",
        "            layer.trainable = False\n",
        "        model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
        "\n",
        "        train_data, val_data, train_label, val_label = train_test_split(data, label, random_state=1, stratify = label)\n",
        "        train_data /= 255\n",
        "        val_data /= 255\n",
        "        early_stopping = tf.keras.callbacks.EarlyStopping(patience=5, monitor=\"val_loss\", min_delta=0.001, restore_best_weights=True, verbose=1)\n",
        "        history = model.fit(train_data, train_label, batch_size=4,\n",
        "                            steps_per_epoch = len(train_data)/4,\n",
        "                            validation_data = (val_data, val_label),\n",
        "                            epochs=120, verbose=1, callbacks=[early_stopping])\n",
        "\n",
        "        models.append(model)\n",
        "        histories.append(history)\n",
        "        prediction = np.argmax(model.predict(test_data[i]),axis=1)\n",
        "        preds.append(prediction)\n",
        "\n",
        "    return models, histories, preds"
      ],
      "metadata": {
        "id": "wTC9qKyRC3Rg"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models, histories, predictions = ACcomparison(\n",
        "    datas=ac_datas, label=label_enc, test_data=ac_tests, test_label=test_label\n",
        ")"
      ],
      "metadata": {
        "id": "5UVzHGP_DD21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "023a0c63-065f-41a5-aaab-993918db2f67"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/120\n",
            "10/10 [==============================] - 7s 230ms/step - loss: 2.0487 - accuracy: 0.0976 - val_loss: 1.7727 - val_accuracy: 0.0714\n",
            "Epoch 2/120\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 1.7183 - accuracy: 0.1707 - val_loss: 1.5501 - val_accuracy: 0.3571\n",
            "Epoch 3/120\n",
            "10/10 [==============================] - 1s 55ms/step - loss: 1.5087 - accuracy: 0.4878 - val_loss: 1.4351 - val_accuracy: 0.5714\n",
            "Epoch 4/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 1.3901 - accuracy: 0.5610 - val_loss: 1.3542 - val_accuracy: 0.5000\n",
            "Epoch 5/120\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 1.3150 - accuracy: 0.5122 - val_loss: 1.3023 - val_accuracy: 0.5000\n",
            "Epoch 6/120\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 1.2413 - accuracy: 0.5854 - val_loss: 1.2621 - val_accuracy: 0.6429\n",
            "Epoch 7/120\n",
            "10/10 [==============================] - 1s 72ms/step - loss: 1.1834 - accuracy: 0.6341 - val_loss: 1.2207 - val_accuracy: 0.5714\n",
            "Epoch 8/120\n",
            "10/10 [==============================] - 1s 72ms/step - loss: 1.1382 - accuracy: 0.6829 - val_loss: 1.1815 - val_accuracy: 0.5714\n",
            "Epoch 9/120\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 1.0879 - accuracy: 0.7317 - val_loss: 1.1487 - val_accuracy: 0.7143\n",
            "Epoch 10/120\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 1.0422 - accuracy: 0.7317 - val_loss: 1.1193 - val_accuracy: 0.7857\n",
            "Epoch 11/120\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 1.0031 - accuracy: 0.8049 - val_loss: 1.0946 - val_accuracy: 0.7857\n",
            "Epoch 12/120\n",
            "10/10 [==============================] - 1s 54ms/step - loss: 0.9651 - accuracy: 0.8537 - val_loss: 1.0661 - val_accuracy: 0.7857\n",
            "Epoch 13/120\n",
            "10/10 [==============================] - 1s 54ms/step - loss: 0.9338 - accuracy: 0.8537 - val_loss: 1.0354 - val_accuracy: 0.7857\n",
            "Epoch 14/120\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 0.9020 - accuracy: 0.8537 - val_loss: 1.0078 - val_accuracy: 0.7857\n",
            "Epoch 15/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.8702 - accuracy: 0.8780 - val_loss: 0.9882 - val_accuracy: 0.7857\n",
            "Epoch 16/120\n",
            "10/10 [==============================] - 1s 60ms/step - loss: 0.8398 - accuracy: 0.9024 - val_loss: 0.9688 - val_accuracy: 0.7857\n",
            "Epoch 17/120\n",
            "10/10 [==============================] - 1s 60ms/step - loss: 0.8118 - accuracy: 0.9512 - val_loss: 0.9479 - val_accuracy: 0.7857\n",
            "Epoch 18/120\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.7910 - accuracy: 0.9512 - val_loss: 0.9220 - val_accuracy: 0.7857\n",
            "Epoch 19/120\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 0.7665 - accuracy: 0.9268 - val_loss: 0.9028 - val_accuracy: 0.7857\n",
            "Epoch 20/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.7435 - accuracy: 0.9512 - val_loss: 0.8902 - val_accuracy: 0.8571\n",
            "Epoch 21/120\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 0.7234 - accuracy: 0.9268 - val_loss: 0.8732 - val_accuracy: 0.7857\n",
            "Epoch 22/120\n",
            "10/10 [==============================] - 1s 54ms/step - loss: 0.7102 - accuracy: 0.8780 - val_loss: 0.8506 - val_accuracy: 0.7857\n",
            "Epoch 23/120\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 0.6872 - accuracy: 0.8780 - val_loss: 0.8368 - val_accuracy: 0.7857\n",
            "Epoch 24/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.6685 - accuracy: 0.9268 - val_loss: 0.8278 - val_accuracy: 0.7857\n",
            "Epoch 25/120\n",
            "10/10 [==============================] - 1s 55ms/step - loss: 0.6490 - accuracy: 0.9268 - val_loss: 0.8151 - val_accuracy: 0.8571\n",
            "Epoch 26/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.6294 - accuracy: 0.9268 - val_loss: 0.7986 - val_accuracy: 0.7857\n",
            "Epoch 27/120\n",
            "10/10 [==============================] - 1s 68ms/step - loss: 0.6155 - accuracy: 0.9268 - val_loss: 0.7904 - val_accuracy: 0.7857\n",
            "Epoch 28/120\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 0.6039 - accuracy: 0.9268 - val_loss: 0.7771 - val_accuracy: 0.7857\n",
            "Epoch 29/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.5887 - accuracy: 0.9268 - val_loss: 0.7685 - val_accuracy: 0.8571\n",
            "Epoch 30/120\n",
            "10/10 [==============================] - 1s 73ms/step - loss: 0.5749 - accuracy: 0.9268 - val_loss: 0.7618 - val_accuracy: 0.7857\n",
            "Epoch 31/120\n",
            "10/10 [==============================] - 1s 70ms/step - loss: 0.5627 - accuracy: 0.9268 - val_loss: 0.7445 - val_accuracy: 0.8571\n",
            "Epoch 32/120\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.5466 - accuracy: 0.9268 - val_loss: 0.7266 - val_accuracy: 0.8571\n",
            "Epoch 33/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.5368 - accuracy: 0.9268 - val_loss: 0.7136 - val_accuracy: 0.8571\n",
            "Epoch 34/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.5266 - accuracy: 0.9268 - val_loss: 0.7001 - val_accuracy: 0.8571\n",
            "Epoch 35/120\n",
            "10/10 [==============================] - 1s 68ms/step - loss: 0.5146 - accuracy: 0.9268 - val_loss: 0.6978 - val_accuracy: 0.8571\n",
            "Epoch 36/120\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.5075 - accuracy: 0.9512 - val_loss: 0.6938 - val_accuracy: 0.8571\n",
            "Epoch 37/120\n",
            "10/10 [==============================] - 1s 60ms/step - loss: 0.4928 - accuracy: 0.9512 - val_loss: 0.6866 - val_accuracy: 0.8571\n",
            "Epoch 38/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.4861 - accuracy: 0.9512 - val_loss: 0.6761 - val_accuracy: 0.7857\n",
            "Epoch 39/120\n",
            "10/10 [==============================] - 1s 54ms/step - loss: 0.4754 - accuracy: 0.9512 - val_loss: 0.6716 - val_accuracy: 0.7857\n",
            "Epoch 40/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.4650 - accuracy: 0.9512 - val_loss: 0.6562 - val_accuracy: 0.8571\n",
            "Epoch 41/120\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 0.4579 - accuracy: 0.9512 - val_loss: 0.6483 - val_accuracy: 0.8571\n",
            "Epoch 42/120\n",
            "10/10 [==============================] - 1s 55ms/step - loss: 0.4451 - accuracy: 0.9512 - val_loss: 0.6258 - val_accuracy: 0.8571\n",
            "Epoch 43/120\n",
            "10/10 [==============================] - 1s 58ms/step - loss: 0.4388 - accuracy: 0.9512 - val_loss: 0.6111 - val_accuracy: 0.8571\n",
            "Epoch 44/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.4308 - accuracy: 0.9268 - val_loss: 0.6084 - val_accuracy: 0.8571\n",
            "Epoch 45/120\n",
            "10/10 [==============================] - 0s 39ms/step - loss: 0.4238 - accuracy: 0.9268 - val_loss: 0.6095 - val_accuracy: 0.7857\n",
            "Epoch 46/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.4148 - accuracy: 0.9512 - val_loss: 0.6068 - val_accuracy: 0.7857\n",
            "Epoch 47/120\n",
            "10/10 [==============================] - 1s 58ms/step - loss: 0.4050 - accuracy: 0.9512 - val_loss: 0.5983 - val_accuracy: 0.8571\n",
            "Epoch 48/120\n",
            "10/10 [==============================] - 0s 46ms/step - loss: 0.3980 - accuracy: 0.9512 - val_loss: 0.5999 - val_accuracy: 0.8571\n",
            "Epoch 49/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.3941 - accuracy: 0.9512 - val_loss: 0.5972 - val_accuracy: 0.8571\n",
            "Epoch 50/120\n",
            "10/10 [==============================] - 1s 72ms/step - loss: 0.3884 - accuracy: 0.9512 - val_loss: 0.5872 - val_accuracy: 0.8571\n",
            "Epoch 51/120\n",
            "10/10 [==============================] - 1s 70ms/step - loss: 0.3816 - accuracy: 0.9512 - val_loss: 0.5747 - val_accuracy: 0.8571\n",
            "Epoch 52/120\n",
            "10/10 [==============================] - 1s 66ms/step - loss: 0.3752 - accuracy: 0.9512 - val_loss: 0.5728 - val_accuracy: 0.8571\n",
            "Epoch 53/120\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 0.3698 - accuracy: 0.9512 - val_loss: 0.5687 - val_accuracy: 0.8571\n",
            "Epoch 54/120\n",
            "10/10 [==============================] - 1s 73ms/step - loss: 0.3614 - accuracy: 0.9512 - val_loss: 0.5649 - val_accuracy: 0.8571\n",
            "Epoch 55/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.3571 - accuracy: 0.9512 - val_loss: 0.5637 - val_accuracy: 0.8571\n",
            "Epoch 56/120\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 0.3506 - accuracy: 0.9512 - val_loss: 0.5556 - val_accuracy: 0.8571\n",
            "Epoch 57/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.3439 - accuracy: 0.9512 - val_loss: 0.5493 - val_accuracy: 0.8571\n",
            "Epoch 58/120\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 0.3390 - accuracy: 0.9512 - val_loss: 0.5452 - val_accuracy: 0.8571\n",
            "Epoch 59/120\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.3353 - accuracy: 0.9512 - val_loss: 0.5430 - val_accuracy: 0.7857\n",
            "Epoch 60/120\n",
            "10/10 [==============================] - 1s 55ms/step - loss: 0.3300 - accuracy: 0.9512 - val_loss: 0.5344 - val_accuracy: 0.8571\n",
            "Epoch 61/120\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 0.3246 - accuracy: 0.9512 - val_loss: 0.5315 - val_accuracy: 0.8571\n",
            "Epoch 62/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.3190 - accuracy: 0.9512 - val_loss: 0.5273 - val_accuracy: 0.8571\n",
            "Epoch 63/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.3154 - accuracy: 0.9512 - val_loss: 0.5247 - val_accuracy: 0.8571\n",
            "Epoch 64/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.3118 - accuracy: 0.9756 - val_loss: 0.5205 - val_accuracy: 0.8571\n",
            "Epoch 65/120\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.3087 - accuracy: 0.9512 - val_loss: 0.5151 - val_accuracy: 0.8571\n",
            "Epoch 66/120\n",
            "10/10 [==============================] - 0s 39ms/step - loss: 0.2996 - accuracy: 0.9756 - val_loss: 0.5174 - val_accuracy: 0.7857\n",
            "Epoch 67/120\n",
            "10/10 [==============================] - 0s 46ms/step - loss: 0.2987 - accuracy: 0.9756 - val_loss: 0.5170 - val_accuracy: 0.7857\n",
            "Epoch 68/120\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.2936 - accuracy: 0.9756 - val_loss: 0.5123 - val_accuracy: 0.7857\n",
            "Epoch 69/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.2884 - accuracy: 0.9756 - val_loss: 0.5047 - val_accuracy: 0.7857\n",
            "Epoch 70/120\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.2842 - accuracy: 0.9756 - val_loss: 0.5013 - val_accuracy: 0.8571\n",
            "Epoch 71/120\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 0.2805 - accuracy: 0.9756 - val_loss: 0.4978 - val_accuracy: 0.8571\n",
            "Epoch 72/120\n",
            "10/10 [==============================] - 1s 53ms/step - loss: 0.2771 - accuracy: 0.9756 - val_loss: 0.4969 - val_accuracy: 0.8571\n",
            "Epoch 73/120\n",
            "10/10 [==============================] - 1s 71ms/step - loss: 0.2739 - accuracy: 0.9756 - val_loss: 0.4895 - val_accuracy: 0.8571\n",
            "Epoch 74/120\n",
            "10/10 [==============================] - 1s 81ms/step - loss: 0.2694 - accuracy: 0.9756 - val_loss: 0.4861 - val_accuracy: 0.8571\n",
            "Epoch 75/120\n",
            "10/10 [==============================] - 1s 75ms/step - loss: 0.2658 - accuracy: 0.9756 - val_loss: 0.4833 - val_accuracy: 0.8571\n",
            "Epoch 76/120\n",
            "10/10 [==============================] - 1s 77ms/step - loss: 0.2627 - accuracy: 0.9756 - val_loss: 0.4768 - val_accuracy: 0.8571\n",
            "Epoch 77/120\n",
            "10/10 [==============================] - 1s 74ms/step - loss: 0.2591 - accuracy: 0.9756 - val_loss: 0.4746 - val_accuracy: 0.8571\n",
            "Epoch 78/120\n",
            "10/10 [==============================] - 1s 70ms/step - loss: 0.2557 - accuracy: 0.9512 - val_loss: 0.4706 - val_accuracy: 0.8571\n",
            "Epoch 79/120\n",
            "10/10 [==============================] - 1s 53ms/step - loss: 0.2520 - accuracy: 0.9756 - val_loss: 0.4706 - val_accuracy: 0.8571\n",
            "Epoch 80/120\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.2488 - accuracy: 0.9756 - val_loss: 0.4658 - val_accuracy: 0.8571\n",
            "Epoch 81/120\n",
            "10/10 [==============================] - 0s 46ms/step - loss: 0.2448 - accuracy: 0.9756 - val_loss: 0.4680 - val_accuracy: 0.7857\n",
            "Epoch 82/120\n",
            "10/10 [==============================] - 0s 46ms/step - loss: 0.2429 - accuracy: 0.9756 - val_loss: 0.4663 - val_accuracy: 0.8571\n",
            "Epoch 83/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.2389 - accuracy: 1.0000 - val_loss: 0.4640 - val_accuracy: 0.7857\n",
            "Epoch 84/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.2364 - accuracy: 0.9756 - val_loss: 0.4597 - val_accuracy: 0.8571\n",
            "Epoch 85/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.2337 - accuracy: 0.9756 - val_loss: 0.4584 - val_accuracy: 0.8571\n",
            "Epoch 86/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.2300 - accuracy: 0.9756 - val_loss: 0.4551 - val_accuracy: 0.8571\n",
            "Epoch 87/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.2269 - accuracy: 1.0000 - val_loss: 0.4511 - val_accuracy: 0.8571\n",
            "Epoch 88/120\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 0.2256 - accuracy: 1.0000 - val_loss: 0.4498 - val_accuracy: 0.7857\n",
            "Epoch 89/120\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.2234 - accuracy: 1.0000 - val_loss: 0.4434 - val_accuracy: 0.8571\n",
            "Epoch 90/120\n",
            "10/10 [==============================] - 0s 42ms/step - loss: 0.2193 - accuracy: 1.0000 - val_loss: 0.4430 - val_accuracy: 0.8571\n",
            "Epoch 91/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.2170 - accuracy: 1.0000 - val_loss: 0.4393 - val_accuracy: 0.8571\n",
            "Epoch 92/120\n",
            "10/10 [==============================] - 0s 45ms/step - loss: 0.2151 - accuracy: 1.0000 - val_loss: 0.4387 - val_accuracy: 0.8571\n",
            "Epoch 93/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.2108 - accuracy: 1.0000 - val_loss: 0.4357 - val_accuracy: 0.8571\n",
            "Epoch 94/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.2092 - accuracy: 1.0000 - val_loss: 0.4346 - val_accuracy: 0.8571\n",
            "Epoch 95/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.2060 - accuracy: 1.0000 - val_loss: 0.4192 - val_accuracy: 0.8571\n",
            "Epoch 96/120\n",
            "10/10 [==============================] - 0s 39ms/step - loss: 0.2028 - accuracy: 1.0000 - val_loss: 0.4197 - val_accuracy: 0.7857\n",
            "Epoch 97/120\n",
            "10/10 [==============================] - 0s 46ms/step - loss: 0.2016 - accuracy: 1.0000 - val_loss: 0.4199 - val_accuracy: 0.7857\n",
            "Epoch 98/120\n",
            "10/10 [==============================] - 1s 52ms/step - loss: 0.1984 - accuracy: 1.0000 - val_loss: 0.4188 - val_accuracy: 0.7857\n",
            "Epoch 99/120\n",
            "10/10 [==============================] - 1s 71ms/step - loss: 0.1952 - accuracy: 1.0000 - val_loss: 0.4182 - val_accuracy: 0.7857\n",
            "Epoch 100/120\n",
            "10/10 [==============================] - 0s 44ms/step - loss: 0.1932 - accuracy: 1.0000 - val_loss: 0.4182 - val_accuracy: 0.7857\n",
            "Epoch 101/120\n",
            "10/10 [==============================] - 1s 52ms/step - loss: 0.1917 - accuracy: 1.0000 - val_loss: 0.4180 - val_accuracy: 0.8571\n",
            "Epoch 102/120\n",
            "10/10 [==============================] - 0s 49ms/step - loss: 0.1892 - accuracy: 1.0000 - val_loss: 0.4184 - val_accuracy: 0.7857\n",
            "Epoch 103/120\n",
            "10/10 [==============================] - 1s 70ms/step - loss: 0.1871 - accuracy: 1.0000 - val_loss: 0.4170 - val_accuracy: 0.7857\n",
            "Epoch 104/120\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.1840 - accuracy: 1.0000 - val_loss: 0.4153 - val_accuracy: 0.8571\n",
            "Epoch 105/120\n",
            "10/10 [==============================] - 1s 55ms/step - loss: 0.1834 - accuracy: 1.0000 - val_loss: 0.4122 - val_accuracy: 0.8571\n",
            "Epoch 106/120\n",
            "10/10 [==============================] - 0s 39ms/step - loss: 0.1807 - accuracy: 1.0000 - val_loss: 0.4128 - val_accuracy: 0.8571\n",
            "Epoch 107/120\n",
            "10/10 [==============================] - 0s 46ms/step - loss: 0.1784 - accuracy: 1.0000 - val_loss: 0.4131 - val_accuracy: 0.8571\n",
            "Epoch 108/120\n",
            "10/10 [==============================] - 0s 45ms/step - loss: 0.1767 - accuracy: 1.0000 - val_loss: 0.4132 - val_accuracy: 0.7857\n",
            "Epoch 109/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.1753 - accuracy: 1.0000 - val_loss: 0.4096 - val_accuracy: 0.7857\n",
            "Epoch 110/120\n",
            "10/10 [==============================] - 0s 39ms/step - loss: 0.1725 - accuracy: 1.0000 - val_loss: 0.4090 - val_accuracy: 0.7857\n",
            "Epoch 111/120\n",
            "10/10 [==============================] - 0s 40ms/step - loss: 0.1710 - accuracy: 1.0000 - val_loss: 0.4107 - val_accuracy: 0.7857\n",
            "Epoch 112/120\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 0.1692 - accuracy: 1.0000 - val_loss: 0.4082 - val_accuracy: 0.7857\n",
            "Epoch 113/120\n",
            "10/10 [==============================] - 0s 46ms/step - loss: 0.1692 - accuracy: 1.0000 - val_loss: 0.4088 - val_accuracy: 0.7857\n",
            "Epoch 114/120\n",
            "10/10 [==============================] - 0s 39ms/step - loss: 0.1657 - accuracy: 1.0000 - val_loss: 0.4076 - val_accuracy: 0.7857\n",
            "Epoch 115/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.1648 - accuracy: 1.0000 - val_loss: 0.4064 - val_accuracy: 0.7857\n",
            "Epoch 116/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.1614 - accuracy: 1.0000 - val_loss: 0.4041 - val_accuracy: 0.8571\n",
            "Epoch 117/120\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.1615 - accuracy: 1.0000 - val_loss: 0.4008 - val_accuracy: 0.8571\n",
            "Epoch 118/120\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.1591 - accuracy: 1.0000 - val_loss: 0.3984 - val_accuracy: 0.8571\n",
            "Epoch 119/120\n",
            "10/10 [==============================] - 0s 48ms/step - loss: 0.1568 - accuracy: 1.0000 - val_loss: 0.4004 - val_accuracy: 0.8571\n",
            "Epoch 120/120\n",
            "10/10 [==============================] - 0s 46ms/step - loss: 0.1551 - accuracy: 1.0000 - val_loss: 0.4016 - val_accuracy: 0.7857\n",
            "2/2 [==============================] - 1s 109ms/step\n",
            "Epoch 1/120\n",
            "10/10 [==============================] - 7s 228ms/step - loss: 1.8509 - accuracy: 0.1951 - val_loss: 1.7240 - val_accuracy: 0.2857\n",
            "Epoch 2/120\n",
            "10/10 [==============================] - 1s 66ms/step - loss: 1.6670 - accuracy: 0.2927 - val_loss: 1.6194 - val_accuracy: 0.5000\n",
            "Epoch 3/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 1.5232 - accuracy: 0.4878 - val_loss: 1.5482 - val_accuracy: 0.5714\n",
            "Epoch 4/120\n",
            "10/10 [==============================] - 1s 60ms/step - loss: 1.4371 - accuracy: 0.5366 - val_loss: 1.4852 - val_accuracy: 0.5714\n",
            "Epoch 5/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 1.3587 - accuracy: 0.5366 - val_loss: 1.4394 - val_accuracy: 0.5000\n",
            "Epoch 6/120\n",
            "10/10 [==============================] - 1s 55ms/step - loss: 1.2886 - accuracy: 0.5610 - val_loss: 1.3934 - val_accuracy: 0.5000\n",
            "Epoch 7/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 1.2226 - accuracy: 0.6341 - val_loss: 1.3615 - val_accuracy: 0.5000\n",
            "Epoch 8/120\n",
            "10/10 [==============================] - 1s 70ms/step - loss: 1.1717 - accuracy: 0.6341 - val_loss: 1.3348 - val_accuracy: 0.4286\n",
            "Epoch 9/120\n",
            "10/10 [==============================] - 1s 68ms/step - loss: 1.1229 - accuracy: 0.6341 - val_loss: 1.2965 - val_accuracy: 0.5000\n",
            "Epoch 10/120\n",
            "10/10 [==============================] - 2s 195ms/step - loss: 1.0839 - accuracy: 0.6341 - val_loss: 1.2621 - val_accuracy: 0.5714\n",
            "Epoch 11/120\n",
            "10/10 [==============================] - 1s 113ms/step - loss: 1.0361 - accuracy: 0.6829 - val_loss: 1.2280 - val_accuracy: 0.6429\n",
            "Epoch 12/120\n",
            "10/10 [==============================] - 1s 105ms/step - loss: 1.0008 - accuracy: 0.7317 - val_loss: 1.2058 - val_accuracy: 0.6429\n",
            "Epoch 13/120\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.9620 - accuracy: 0.7317 - val_loss: 1.1766 - val_accuracy: 0.7143\n",
            "Epoch 14/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.9300 - accuracy: 0.7561 - val_loss: 1.1528 - val_accuracy: 0.7143\n",
            "Epoch 15/120\n",
            "10/10 [==============================] - 1s 55ms/step - loss: 0.8988 - accuracy: 0.7561 - val_loss: 1.1261 - val_accuracy: 0.7143\n",
            "Epoch 16/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.8757 - accuracy: 0.7561 - val_loss: 1.1095 - val_accuracy: 0.6429\n",
            "Epoch 17/120\n",
            "10/10 [==============================] - 1s 58ms/step - loss: 0.8513 - accuracy: 0.8049 - val_loss: 1.0910 - val_accuracy: 0.6429\n",
            "Epoch 18/120\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 0.8211 - accuracy: 0.8537 - val_loss: 1.0690 - val_accuracy: 0.7143\n",
            "Epoch 19/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.7965 - accuracy: 0.8049 - val_loss: 1.0478 - val_accuracy: 0.7143\n",
            "Epoch 20/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.7773 - accuracy: 0.8049 - val_loss: 1.0174 - val_accuracy: 0.7143\n",
            "Epoch 21/120\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 0.7523 - accuracy: 0.7805 - val_loss: 1.0043 - val_accuracy: 0.7143\n",
            "Epoch 22/120\n",
            "10/10 [==============================] - 1s 55ms/step - loss: 0.7343 - accuracy: 0.8049 - val_loss: 0.9945 - val_accuracy: 0.7143\n",
            "Epoch 23/120\n",
            "10/10 [==============================] - 1s 55ms/step - loss: 0.7169 - accuracy: 0.8537 - val_loss: 0.9825 - val_accuracy: 0.7143\n",
            "Epoch 24/120\n",
            "10/10 [==============================] - 1s 55ms/step - loss: 0.6974 - accuracy: 0.8780 - val_loss: 0.9672 - val_accuracy: 0.7143\n",
            "Epoch 25/120\n",
            "10/10 [==============================] - 1s 60ms/step - loss: 0.6826 - accuracy: 0.9024 - val_loss: 0.9586 - val_accuracy: 0.7143\n",
            "Epoch 26/120\n",
            "10/10 [==============================] - 1s 60ms/step - loss: 0.6677 - accuracy: 0.9024 - val_loss: 0.9444 - val_accuracy: 0.7143\n",
            "Epoch 27/120\n",
            "10/10 [==============================] - 1s 60ms/step - loss: 0.6495 - accuracy: 0.9024 - val_loss: 0.9307 - val_accuracy: 0.7143\n",
            "Epoch 28/120\n",
            "10/10 [==============================] - 1s 70ms/step - loss: 0.6348 - accuracy: 0.9268 - val_loss: 0.9176 - val_accuracy: 0.7143\n",
            "Epoch 29/120\n",
            "10/10 [==============================] - 1s 70ms/step - loss: 0.6202 - accuracy: 0.9268 - val_loss: 0.8959 - val_accuracy: 0.7143\n",
            "Epoch 30/120\n",
            "10/10 [==============================] - 1s 68ms/step - loss: 0.6058 - accuracy: 0.9024 - val_loss: 0.8845 - val_accuracy: 0.7143\n",
            "Epoch 31/120\n",
            "10/10 [==============================] - 1s 72ms/step - loss: 0.5943 - accuracy: 0.8780 - val_loss: 0.8682 - val_accuracy: 0.7857\n",
            "Epoch 32/120\n",
            "10/10 [==============================] - 1s 72ms/step - loss: 0.5797 - accuracy: 0.9024 - val_loss: 0.8659 - val_accuracy: 0.7143\n",
            "Epoch 33/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.5667 - accuracy: 0.9268 - val_loss: 0.8505 - val_accuracy: 0.7857\n",
            "Epoch 34/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.5566 - accuracy: 0.9268 - val_loss: 0.8385 - val_accuracy: 0.7857\n",
            "Epoch 35/120\n",
            "10/10 [==============================] - 1s 55ms/step - loss: 0.5472 - accuracy: 0.9268 - val_loss: 0.8347 - val_accuracy: 0.8571\n",
            "Epoch 36/120\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.5325 - accuracy: 0.9756 - val_loss: 0.8282 - val_accuracy: 0.7857\n",
            "Epoch 37/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.5222 - accuracy: 0.9756 - val_loss: 0.8214 - val_accuracy: 0.7857\n",
            "Epoch 38/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.5099 - accuracy: 0.9756 - val_loss: 0.8152 - val_accuracy: 0.7857\n",
            "Epoch 39/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.5029 - accuracy: 0.9756 - val_loss: 0.8110 - val_accuracy: 0.7857\n",
            "Epoch 40/120\n",
            "10/10 [==============================] - 1s 55ms/step - loss: 0.4919 - accuracy: 0.9512 - val_loss: 0.7981 - val_accuracy: 0.7857\n",
            "Epoch 41/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.4805 - accuracy: 0.9756 - val_loss: 0.7885 - val_accuracy: 0.7857\n",
            "Epoch 42/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.4743 - accuracy: 0.9756 - val_loss: 0.7832 - val_accuracy: 0.7857\n",
            "Epoch 43/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.4629 - accuracy: 0.9756 - val_loss: 0.7747 - val_accuracy: 0.7857\n",
            "Epoch 44/120\n",
            "10/10 [==============================] - 1s 55ms/step - loss: 0.4532 - accuracy: 0.9756 - val_loss: 0.7538 - val_accuracy: 0.8571\n",
            "Epoch 45/120\n",
            "10/10 [==============================] - 0s 40ms/step - loss: 0.4462 - accuracy: 0.9512 - val_loss: 0.7542 - val_accuracy: 0.8571\n",
            "Epoch 46/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.4375 - accuracy: 0.9512 - val_loss: 0.7470 - val_accuracy: 0.8571\n",
            "Epoch 47/120\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.4325 - accuracy: 0.9512 - val_loss: 0.7377 - val_accuracy: 0.8571\n",
            "Epoch 48/120\n",
            "10/10 [==============================] - 1s 55ms/step - loss: 0.4204 - accuracy: 0.9512 - val_loss: 0.7186 - val_accuracy: 0.8571\n",
            "Epoch 49/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.4135 - accuracy: 0.9756 - val_loss: 0.7153 - val_accuracy: 0.8571\n",
            "Epoch 50/120\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 0.4085 - accuracy: 0.9756 - val_loss: 0.7118 - val_accuracy: 0.8571\n",
            "Epoch 51/120\n",
            "10/10 [==============================] - 0s 50ms/step - loss: 0.4020 - accuracy: 0.9756 - val_loss: 0.7135 - val_accuracy: 0.7857\n",
            "Epoch 52/120\n",
            "10/10 [==============================] - 1s 71ms/step - loss: 0.3965 - accuracy: 0.9512 - val_loss: 0.6957 - val_accuracy: 0.8571\n",
            "Epoch 53/120\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.3888 - accuracy: 0.9512 - val_loss: 0.6884 - val_accuracy: 0.9286\n",
            "Epoch 54/120\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 0.3794 - accuracy: 0.9756 - val_loss: 0.6843 - val_accuracy: 0.9286\n",
            "Epoch 55/120\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 0.3728 - accuracy: 0.9756 - val_loss: 0.6826 - val_accuracy: 0.8571\n",
            "Epoch 56/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.3676 - accuracy: 0.9756 - val_loss: 0.6810 - val_accuracy: 0.8571\n",
            "Epoch 57/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.3628 - accuracy: 0.9756 - val_loss: 0.6669 - val_accuracy: 0.9286\n",
            "Epoch 58/120\n",
            "10/10 [==============================] - 1s 54ms/step - loss: 0.3555 - accuracy: 0.9512 - val_loss: 0.6603 - val_accuracy: 0.9286\n",
            "Epoch 59/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.3519 - accuracy: 0.9756 - val_loss: 0.6575 - val_accuracy: 0.9286\n",
            "Epoch 60/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.3466 - accuracy: 0.9756 - val_loss: 0.6524 - val_accuracy: 0.8571\n",
            "Epoch 61/120\n",
            "10/10 [==============================] - 1s 60ms/step - loss: 0.3395 - accuracy: 0.9756 - val_loss: 0.6497 - val_accuracy: 0.8571\n",
            "Epoch 62/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.3357 - accuracy: 0.9512 - val_loss: 0.6419 - val_accuracy: 0.9286\n",
            "Epoch 63/120\n",
            "10/10 [==============================] - 0s 41ms/step - loss: 0.3282 - accuracy: 0.9756 - val_loss: 0.6428 - val_accuracy: 0.9286\n",
            "Epoch 64/120\n",
            "10/10 [==============================] - 0s 39ms/step - loss: 0.3233 - accuracy: 0.9756 - val_loss: 0.6446 - val_accuracy: 0.9286\n",
            "Epoch 65/120\n",
            "10/10 [==============================] - 0s 47ms/step - loss: 0.3190 - accuracy: 0.9756 - val_loss: 0.6421 - val_accuracy: 0.9286\n",
            "Epoch 66/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.3126 - accuracy: 0.9756 - val_loss: 0.6327 - val_accuracy: 0.9286\n",
            "Epoch 67/120\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.3076 - accuracy: 0.9756 - val_loss: 0.6269 - val_accuracy: 0.9286\n",
            "Epoch 68/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.3043 - accuracy: 1.0000 - val_loss: 0.6206 - val_accuracy: 0.9286\n",
            "Epoch 69/120\n",
            "10/10 [==============================] - 0s 40ms/step - loss: 0.3003 - accuracy: 0.9756 - val_loss: 0.6238 - val_accuracy: 0.9286\n",
            "Epoch 70/120\n",
            "10/10 [==============================] - 0s 40ms/step - loss: 0.2960 - accuracy: 0.9756 - val_loss: 0.6198 - val_accuracy: 0.9286\n",
            "Epoch 71/120\n",
            "10/10 [==============================] - 0s 46ms/step - loss: 0.2926 - accuracy: 1.0000 - val_loss: 0.6233 - val_accuracy: 0.8571\n",
            "Epoch 72/120\n",
            "10/10 [==============================] - 0s 47ms/step - loss: 0.2867 - accuracy: 1.0000 - val_loss: 0.6203 - val_accuracy: 0.8571\n",
            "Epoch 73/120\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.2836 - accuracy: 1.0000 - val_loss: 0.6125 - val_accuracy: 0.9286\n",
            "Epoch 74/120\n",
            "10/10 [==============================] - 0s 50ms/step - loss: 0.2773 - accuracy: 1.0000 - val_loss: 0.6145 - val_accuracy: 0.9286\n",
            "Epoch 75/120\n",
            "10/10 [==============================] - 1s 71ms/step - loss: 0.2742 - accuracy: 1.0000 - val_loss: 0.6015 - val_accuracy: 0.9286\n",
            "Epoch 76/120\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.2707 - accuracy: 1.0000 - val_loss: 0.5968 - val_accuracy: 0.9286\n",
            "Epoch 77/120\n",
            "10/10 [==============================] - 1s 71ms/step - loss: 0.2688 - accuracy: 1.0000 - val_loss: 0.5925 - val_accuracy: 0.9286\n",
            "Epoch 78/120\n",
            "10/10 [==============================] - 1s 51ms/step - loss: 0.2630 - accuracy: 1.0000 - val_loss: 0.5918 - val_accuracy: 0.9286\n",
            "Epoch 79/120\n",
            "10/10 [==============================] - 1s 52ms/step - loss: 0.2600 - accuracy: 1.0000 - val_loss: 0.5922 - val_accuracy: 0.9286\n",
            "Epoch 80/120\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.2561 - accuracy: 1.0000 - val_loss: 0.5908 - val_accuracy: 0.9286\n",
            "Epoch 81/120\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.2539 - accuracy: 1.0000 - val_loss: 0.5893 - val_accuracy: 0.9286\n",
            "Epoch 82/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.2504 - accuracy: 1.0000 - val_loss: 0.5846 - val_accuracy: 0.9286\n",
            "Epoch 83/120\n",
            "10/10 [==============================] - 1s 55ms/step - loss: 0.2471 - accuracy: 1.0000 - val_loss: 0.5810 - val_accuracy: 0.9286\n",
            "Epoch 84/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.2433 - accuracy: 1.0000 - val_loss: 0.5784 - val_accuracy: 0.8571\n",
            "Epoch 85/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.2401 - accuracy: 1.0000 - val_loss: 0.5768 - val_accuracy: 0.9286\n",
            "Epoch 86/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.2380 - accuracy: 1.0000 - val_loss: 0.5635 - val_accuracy: 0.9286\n",
            "Epoch 87/120\n",
            "10/10 [==============================] - 0s 47ms/step - loss: 0.2330 - accuracy: 1.0000 - val_loss: 0.5659 - val_accuracy: 0.9286\n",
            "Epoch 88/120\n",
            "10/10 [==============================] - 0s 40ms/step - loss: 0.2310 - accuracy: 1.0000 - val_loss: 0.5662 - val_accuracy: 0.9286\n",
            "Epoch 89/120\n",
            "10/10 [==============================] - 0s 46ms/step - loss: 0.2283 - accuracy: 1.0000 - val_loss: 0.5650 - val_accuracy: 0.9286\n",
            "Epoch 90/120\n",
            "10/10 [==============================] - 0s 39ms/step - loss: 0.2252 - accuracy: 1.0000 - val_loss: 0.5630 - val_accuracy: 0.9286\n",
            "Epoch 91/120\n",
            "10/10 [==============================] - 1s 55ms/step - loss: 0.2239 - accuracy: 1.0000 - val_loss: 0.5518 - val_accuracy: 0.9286\n",
            "Epoch 92/120\n",
            "10/10 [==============================] - 1s 60ms/step - loss: 0.2195 - accuracy: 1.0000 - val_loss: 0.5483 - val_accuracy: 0.9286\n",
            "Epoch 93/120\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 0.2175 - accuracy: 1.0000 - val_loss: 0.5471 - val_accuracy: 0.9286\n",
            "Epoch 94/120\n",
            "10/10 [==============================] - 0s 46ms/step - loss: 0.2144 - accuracy: 1.0000 - val_loss: 0.5470 - val_accuracy: 0.9286\n",
            "Epoch 95/120\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.2119 - accuracy: 1.0000 - val_loss: 0.5447 - val_accuracy: 0.9286\n",
            "Epoch 96/120\n",
            "10/10 [==============================] - 0s 46ms/step - loss: 0.2093 - accuracy: 1.0000 - val_loss: 0.5455 - val_accuracy: 0.9286\n",
            "Epoch 97/120\n",
            "10/10 [==============================] - 1s 60ms/step - loss: 0.2071 - accuracy: 1.0000 - val_loss: 0.5390 - val_accuracy: 0.9286\n",
            "Epoch 98/120\n",
            "10/10 [==============================] - 0s 39ms/step - loss: 0.2030 - accuracy: 1.0000 - val_loss: 0.5404 - val_accuracy: 0.9286\n",
            "Epoch 99/120\n",
            "10/10 [==============================] - 0s 40ms/step - loss: 0.2020 - accuracy: 1.0000 - val_loss: 0.5409 - val_accuracy: 0.9286\n",
            "Epoch 100/120\n",
            "10/10 [==============================] - 1s 68ms/step - loss: 0.2006 - accuracy: 1.0000 - val_loss: 0.5373 - val_accuracy: 0.9286\n",
            "Epoch 101/120\n",
            "10/10 [==============================] - 1s 70ms/step - loss: 0.1982 - accuracy: 1.0000 - val_loss: 0.5359 - val_accuracy: 0.9286\n",
            "Epoch 102/120\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 0.1965 - accuracy: 1.0000 - val_loss: 0.5229 - val_accuracy: 0.9286\n",
            "Epoch 103/120\n",
            "10/10 [==============================] - 0s 49ms/step - loss: 0.1943 - accuracy: 1.0000 - val_loss: 0.5250 - val_accuracy: 0.8571\n",
            "Epoch 104/120\n",
            "10/10 [==============================] - 0s 49ms/step - loss: 0.1920 - accuracy: 1.0000 - val_loss: 0.5324 - val_accuracy: 0.8571\n",
            "Epoch 105/120\n",
            "10/10 [==============================] - 0s 50ms/step - loss: 0.1900 - accuracy: 1.0000 - val_loss: 0.5292 - val_accuracy: 0.8571\n",
            "Epoch 106/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.1875 - accuracy: 1.0000 - val_loss: 0.5186 - val_accuracy: 0.8571\n",
            "Epoch 107/120\n",
            "10/10 [==============================] - 1s 55ms/step - loss: 0.1837 - accuracy: 1.0000 - val_loss: 0.5163 - val_accuracy: 0.9286\n",
            "Epoch 108/120\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 0.1819 - accuracy: 1.0000 - val_loss: 0.5029 - val_accuracy: 0.9286\n",
            "Epoch 109/120\n",
            "10/10 [==============================] - 1s 55ms/step - loss: 0.1794 - accuracy: 1.0000 - val_loss: 0.5017 - val_accuracy: 0.9286\n",
            "Epoch 110/120\n",
            "10/10 [==============================] - 0s 41ms/step - loss: 0.1776 - accuracy: 1.0000 - val_loss: 0.5026 - val_accuracy: 0.9286\n",
            "Epoch 111/120\n",
            "10/10 [==============================] - 0s 40ms/step - loss: 0.1752 - accuracy: 1.0000 - val_loss: 0.5017 - val_accuracy: 0.9286\n",
            "Epoch 112/120\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 0.1742 - accuracy: 1.0000 - val_loss: 0.4999 - val_accuracy: 0.9286\n",
            "Epoch 113/120\n",
            "10/10 [==============================] - 0s 45ms/step - loss: 0.1717 - accuracy: 1.0000 - val_loss: 0.5026 - val_accuracy: 0.9286\n",
            "Epoch 114/120\n",
            "10/10 [==============================] - 0s 46ms/step - loss: 0.1694 - accuracy: 1.0000 - val_loss: 0.5041 - val_accuracy: 0.9286\n",
            "Epoch 115/120\n",
            "10/10 [==============================] - 0s 48ms/step - loss: 0.1694 - accuracy: 1.0000 - val_loss: 0.5047 - val_accuracy: 0.9286\n",
            "Epoch 116/120\n",
            "10/10 [==============================] - 0s 46ms/step - loss: 0.1653 - accuracy: 1.0000 - val_loss: 0.5050 - val_accuracy: 0.8571\n",
            "Epoch 117/120\n",
            "10/10 [============================>.] - ETA: 0s - loss: 0.1567 - accuracy: 1.0000Restoring model weights from the end of the best epoch: 112.\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.1648 - accuracy: 1.0000 - val_loss: 0.5033 - val_accuracy: 0.8571\n",
            "Epoch 117: early stopping\n",
            "2/2 [==============================] - 1s 107ms/step\n",
            "Epoch 1/120\n",
            "10/10 [==============================] - 7s 222ms/step - loss: 1.8214 - accuracy: 0.0976 - val_loss: 1.7190 - val_accuracy: 0.1429\n",
            "Epoch 2/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 1.6239 - accuracy: 0.3902 - val_loss: 1.5981 - val_accuracy: 0.3571\n",
            "Epoch 3/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 1.5056 - accuracy: 0.4634 - val_loss: 1.5283 - val_accuracy: 0.3571\n",
            "Epoch 4/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 1.4368 - accuracy: 0.4146 - val_loss: 1.4983 - val_accuracy: 0.3571\n",
            "Epoch 5/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 1.3688 - accuracy: 0.4390 - val_loss: 1.4634 - val_accuracy: 0.3571\n",
            "Epoch 6/120\n",
            "10/10 [==============================] - 1s 55ms/step - loss: 1.3128 - accuracy: 0.4634 - val_loss: 1.4174 - val_accuracy: 0.4286\n",
            "Epoch 7/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 1.2631 - accuracy: 0.5122 - val_loss: 1.3804 - val_accuracy: 0.4286\n",
            "Epoch 8/120\n",
            "10/10 [==============================] - 1s 54ms/step - loss: 1.2219 - accuracy: 0.5122 - val_loss: 1.3480 - val_accuracy: 0.4286\n",
            "Epoch 9/120\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 1.1790 - accuracy: 0.6098 - val_loss: 1.3173 - val_accuracy: 0.4286\n",
            "Epoch 10/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 1.1484 - accuracy: 0.6098 - val_loss: 1.3027 - val_accuracy: 0.4286\n",
            "Epoch 11/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 1.1054 - accuracy: 0.6585 - val_loss: 1.2755 - val_accuracy: 0.5714\n",
            "Epoch 12/120\n",
            "10/10 [==============================] - 1s 71ms/step - loss: 1.0774 - accuracy: 0.6829 - val_loss: 1.2416 - val_accuracy: 0.5714\n",
            "Epoch 13/120\n",
            "10/10 [==============================] - 1s 68ms/step - loss: 1.0458 - accuracy: 0.7317 - val_loss: 1.2070 - val_accuracy: 0.6429\n",
            "Epoch 14/120\n",
            "10/10 [==============================] - 1s 72ms/step - loss: 1.0182 - accuracy: 0.7805 - val_loss: 1.1886 - val_accuracy: 0.7857\n",
            "Epoch 15/120\n",
            "10/10 [==============================] - 1s 71ms/step - loss: 0.9874 - accuracy: 0.8049 - val_loss: 1.1679 - val_accuracy: 0.7143\n",
            "Epoch 16/120\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.9682 - accuracy: 0.7805 - val_loss: 1.1449 - val_accuracy: 0.6429\n",
            "Epoch 17/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.9335 - accuracy: 0.7805 - val_loss: 1.1328 - val_accuracy: 0.7143\n",
            "Epoch 18/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.9105 - accuracy: 0.7805 - val_loss: 1.1002 - val_accuracy: 0.7857\n",
            "Epoch 19/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.8849 - accuracy: 0.7805 - val_loss: 1.0816 - val_accuracy: 0.7857\n",
            "Epoch 20/120\n",
            "10/10 [==============================] - 1s 55ms/step - loss: 0.8663 - accuracy: 0.7805 - val_loss: 1.0675 - val_accuracy: 0.8571\n",
            "Epoch 21/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.8442 - accuracy: 0.7805 - val_loss: 1.0483 - val_accuracy: 0.8571\n",
            "Epoch 22/120\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.8251 - accuracy: 0.8049 - val_loss: 1.0341 - val_accuracy: 0.7857\n",
            "Epoch 23/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.8065 - accuracy: 0.8537 - val_loss: 1.0125 - val_accuracy: 0.8571\n",
            "Epoch 24/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.7873 - accuracy: 0.8537 - val_loss: 1.0033 - val_accuracy: 0.7143\n",
            "Epoch 25/120\n",
            "10/10 [==============================] - 1s 68ms/step - loss: 0.7706 - accuracy: 0.8537 - val_loss: 0.9792 - val_accuracy: 0.7143\n",
            "Epoch 26/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.7522 - accuracy: 0.8537 - val_loss: 0.9600 - val_accuracy: 0.7857\n",
            "Epoch 27/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.7375 - accuracy: 0.8537 - val_loss: 0.9413 - val_accuracy: 0.8571\n",
            "Epoch 28/120\n",
            "10/10 [==============================] - 1s 66ms/step - loss: 0.7201 - accuracy: 0.8780 - val_loss: 0.9253 - val_accuracy: 0.9286\n",
            "Epoch 29/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.7094 - accuracy: 0.8780 - val_loss: 0.9186 - val_accuracy: 0.9286\n",
            "Epoch 30/120\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 0.6921 - accuracy: 0.9024 - val_loss: 0.9063 - val_accuracy: 0.9286\n",
            "Epoch 31/120\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 0.6778 - accuracy: 0.9024 - val_loss: 0.8974 - val_accuracy: 0.9286\n",
            "Epoch 32/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.6656 - accuracy: 0.9268 - val_loss: 0.8857 - val_accuracy: 0.9286\n",
            "Epoch 33/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.6535 - accuracy: 0.9512 - val_loss: 0.8775 - val_accuracy: 0.9286\n",
            "Epoch 34/120\n",
            "10/10 [==============================] - 1s 68ms/step - loss: 0.6435 - accuracy: 0.9512 - val_loss: 0.8600 - val_accuracy: 0.9286\n",
            "Epoch 35/120\n",
            "10/10 [==============================] - 1s 72ms/step - loss: 0.6274 - accuracy: 0.9512 - val_loss: 0.8507 - val_accuracy: 0.9286\n",
            "Epoch 36/120\n",
            "10/10 [==============================] - 1s 66ms/step - loss: 0.6168 - accuracy: 0.9512 - val_loss: 0.8338 - val_accuracy: 0.9286\n",
            "Epoch 37/120\n",
            "10/10 [==============================] - 1s 72ms/step - loss: 0.6060 - accuracy: 0.9268 - val_loss: 0.8209 - val_accuracy: 0.9286\n",
            "Epoch 38/120\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.5963 - accuracy: 0.9268 - val_loss: 0.8132 - val_accuracy: 0.9286\n",
            "Epoch 39/120\n",
            "10/10 [==============================] - 1s 68ms/step - loss: 0.5874 - accuracy: 0.9268 - val_loss: 0.8016 - val_accuracy: 0.9286\n",
            "Epoch 40/120\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.5794 - accuracy: 0.9268 - val_loss: 0.7835 - val_accuracy: 0.9286\n",
            "Epoch 41/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.5662 - accuracy: 0.9268 - val_loss: 0.7661 - val_accuracy: 0.9286\n",
            "Epoch 42/120\n",
            "10/10 [==============================] - 1s 58ms/step - loss: 0.5551 - accuracy: 0.9268 - val_loss: 0.7613 - val_accuracy: 0.9286\n",
            "Epoch 43/120\n",
            "10/10 [==============================] - 1s 58ms/step - loss: 0.5470 - accuracy: 0.9268 - val_loss: 0.7594 - val_accuracy: 0.9286\n",
            "Epoch 44/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.5375 - accuracy: 0.9268 - val_loss: 0.7515 - val_accuracy: 0.9286\n",
            "Epoch 45/120\n",
            "10/10 [==============================] - 1s 59ms/step - loss: 0.5274 - accuracy: 0.9512 - val_loss: 0.7384 - val_accuracy: 0.9286\n",
            "Epoch 46/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.5184 - accuracy: 0.9268 - val_loss: 0.7270 - val_accuracy: 0.9286\n",
            "Epoch 47/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.5117 - accuracy: 0.9268 - val_loss: 0.7201 - val_accuracy: 0.9286\n",
            "Epoch 48/120\n",
            "10/10 [==============================] - 1s 58ms/step - loss: 0.5025 - accuracy: 0.9268 - val_loss: 0.7094 - val_accuracy: 0.9286\n",
            "Epoch 49/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.4936 - accuracy: 0.9512 - val_loss: 0.7057 - val_accuracy: 0.9286\n",
            "Epoch 50/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.4882 - accuracy: 0.9512 - val_loss: 0.6984 - val_accuracy: 0.9286\n",
            "Epoch 51/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.4802 - accuracy: 0.9512 - val_loss: 0.6935 - val_accuracy: 0.9286\n",
            "Epoch 52/120\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.4729 - accuracy: 0.9512 - val_loss: 0.6808 - val_accuracy: 0.9286\n",
            "Epoch 53/120\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.4661 - accuracy: 0.9512 - val_loss: 0.6724 - val_accuracy: 0.9286\n",
            "Epoch 54/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.4573 - accuracy: 0.9512 - val_loss: 0.6671 - val_accuracy: 0.9286\n",
            "Epoch 55/120\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.4502 - accuracy: 0.9512 - val_loss: 0.6629 - val_accuracy: 0.9286\n",
            "Epoch 56/120\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.4458 - accuracy: 0.9268 - val_loss: 0.6485 - val_accuracy: 0.9286\n",
            "Epoch 57/120\n",
            "10/10 [==============================] - 1s 68ms/step - loss: 0.4394 - accuracy: 0.9268 - val_loss: 0.6343 - val_accuracy: 0.9286\n",
            "Epoch 58/120\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.4319 - accuracy: 0.9268 - val_loss: 0.6326 - val_accuracy: 0.9286\n",
            "Epoch 59/120\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 0.4254 - accuracy: 0.9268 - val_loss: 0.6277 - val_accuracy: 0.9286\n",
            "Epoch 60/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.4190 - accuracy: 0.9268 - val_loss: 0.6208 - val_accuracy: 0.9286\n",
            "Epoch 61/120\n",
            "10/10 [==============================] - 1s 66ms/step - loss: 0.4139 - accuracy: 0.9268 - val_loss: 0.6169 - val_accuracy: 0.9286\n",
            "Epoch 62/120\n",
            "10/10 [==============================] - 1s 53ms/step - loss: 0.4067 - accuracy: 0.9512 - val_loss: 0.6160 - val_accuracy: 0.9286\n",
            "Epoch 63/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.4022 - accuracy: 0.9512 - val_loss: 0.6065 - val_accuracy: 0.9286\n",
            "Epoch 64/120\n",
            "10/10 [==============================] - 1s 58ms/step - loss: 0.3956 - accuracy: 0.9512 - val_loss: 0.5994 - val_accuracy: 0.9286\n",
            "Epoch 65/120\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.3908 - accuracy: 0.9512 - val_loss: 0.5977 - val_accuracy: 0.9286\n",
            "Epoch 66/120\n",
            "10/10 [==============================] - 0s 47ms/step - loss: 0.3861 - accuracy: 0.9512 - val_loss: 0.5978 - val_accuracy: 0.9286\n",
            "Epoch 67/120\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.3816 - accuracy: 0.9512 - val_loss: 0.5822 - val_accuracy: 0.9286\n",
            "Epoch 68/120\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.3749 - accuracy: 0.9512 - val_loss: 0.5778 - val_accuracy: 0.9286\n",
            "Epoch 69/120\n",
            "10/10 [==============================] - 0s 47ms/step - loss: 0.3706 - accuracy: 0.9512 - val_loss: 0.5784 - val_accuracy: 0.9286\n",
            "Epoch 70/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.3650 - accuracy: 0.9512 - val_loss: 0.5712 - val_accuracy: 0.9286\n",
            "Epoch 71/120\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.3654 - accuracy: 0.9512 - val_loss: 0.5634 - val_accuracy: 0.9286\n",
            "Epoch 72/120\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.3543 - accuracy: 0.9512 - val_loss: 0.5556 - val_accuracy: 0.9286\n",
            "Epoch 73/120\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.3500 - accuracy: 0.9512 - val_loss: 0.5504 - val_accuracy: 0.9286\n",
            "Epoch 74/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.3461 - accuracy: 0.9512 - val_loss: 0.5448 - val_accuracy: 0.9286\n",
            "Epoch 75/120\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.3417 - accuracy: 0.9268 - val_loss: 0.5419 - val_accuracy: 0.9286\n",
            "Epoch 76/120\n",
            "10/10 [==============================] - 0s 40ms/step - loss: 0.3368 - accuracy: 0.9512 - val_loss: 0.5428 - val_accuracy: 0.9286\n",
            "Epoch 77/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.3338 - accuracy: 0.9512 - val_loss: 0.5395 - val_accuracy: 0.9286\n",
            "Epoch 78/120\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.3295 - accuracy: 0.9512 - val_loss: 0.5352 - val_accuracy: 0.9286\n",
            "Epoch 79/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.3269 - accuracy: 0.9268 - val_loss: 0.5241 - val_accuracy: 0.9286\n",
            "Epoch 80/120\n",
            "10/10 [==============================] - 1s 68ms/step - loss: 0.3225 - accuracy: 0.9512 - val_loss: 0.5218 - val_accuracy: 0.9286\n",
            "Epoch 81/120\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.3163 - accuracy: 0.9512 - val_loss: 0.5152 - val_accuracy: 0.9286\n",
            "Epoch 82/120\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.3130 - accuracy: 0.9512 - val_loss: 0.5052 - val_accuracy: 0.9286\n",
            "Epoch 83/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.3069 - accuracy: 0.9512 - val_loss: 0.4867 - val_accuracy: 0.9286\n",
            "Epoch 84/120\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.3087 - accuracy: 0.9512 - val_loss: 0.4802 - val_accuracy: 0.9286\n",
            "Epoch 85/120\n",
            "10/10 [==============================] - 1s 52ms/step - loss: 0.3015 - accuracy: 0.9512 - val_loss: 0.4805 - val_accuracy: 0.9286\n",
            "Epoch 86/120\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.2984 - accuracy: 0.9512 - val_loss: 0.4778 - val_accuracy: 0.9286\n",
            "Epoch 87/120\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 0.2933 - accuracy: 0.9512 - val_loss: 0.4757 - val_accuracy: 0.9286\n",
            "Epoch 88/120\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.2900 - accuracy: 0.9512 - val_loss: 0.4725 - val_accuracy: 0.9286\n",
            "Epoch 89/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.2877 - accuracy: 0.9512 - val_loss: 0.4700 - val_accuracy: 0.9286\n",
            "Epoch 90/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.2821 - accuracy: 0.9512 - val_loss: 0.4678 - val_accuracy: 0.9286\n",
            "Epoch 91/120\n",
            "10/10 [==============================] - 0s 40ms/step - loss: 0.2817 - accuracy: 0.9756 - val_loss: 0.4732 - val_accuracy: 0.9286\n",
            "Epoch 92/120\n",
            "10/10 [==============================] - 0s 40ms/step - loss: 0.2816 - accuracy: 0.9756 - val_loss: 0.4751 - val_accuracy: 0.9286\n",
            "Epoch 93/120\n",
            "10/10 [==============================] - 0s 40ms/step - loss: 0.2798 - accuracy: 0.9756 - val_loss: 0.4798 - val_accuracy: 0.9286\n",
            "Epoch 94/120\n",
            "10/10 [==============================] - 0s 46ms/step - loss: 0.2760 - accuracy: 0.9756 - val_loss: 0.4729 - val_accuracy: 0.9286\n",
            "Epoch 95/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.2718 - accuracy: 0.9756 - val_loss: 0.4650 - val_accuracy: 0.9286\n",
            "Epoch 96/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.2678 - accuracy: 0.9756 - val_loss: 0.4617 - val_accuracy: 0.9286\n",
            "Epoch 97/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.2637 - accuracy: 0.9756 - val_loss: 0.4602 - val_accuracy: 0.9286\n",
            "Epoch 98/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.2627 - accuracy: 0.9756 - val_loss: 0.4554 - val_accuracy: 0.9286\n",
            "Epoch 99/120\n",
            "10/10 [==============================] - 1s 55ms/step - loss: 0.2577 - accuracy: 0.9756 - val_loss: 0.4532 - val_accuracy: 0.9286\n",
            "Epoch 100/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.2559 - accuracy: 0.9756 - val_loss: 0.4458 - val_accuracy: 0.9286\n",
            "Epoch 101/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.2526 - accuracy: 0.9756 - val_loss: 0.4422 - val_accuracy: 0.9286\n",
            "Epoch 102/120\n",
            "10/10 [==============================] - 1s 59ms/step - loss: 0.2495 - accuracy: 0.9756 - val_loss: 0.4411 - val_accuracy: 0.9286\n",
            "Epoch 103/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.2472 - accuracy: 0.9756 - val_loss: 0.4390 - val_accuracy: 0.9286\n",
            "Epoch 104/120\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.2443 - accuracy: 0.9756 - val_loss: 0.4334 - val_accuracy: 0.9286\n",
            "Epoch 105/120\n",
            "10/10 [==============================] - 0s 49ms/step - loss: 0.2415 - accuracy: 1.0000 - val_loss: 0.4329 - val_accuracy: 0.9286\n",
            "Epoch 106/120\n",
            "10/10 [==============================] - 1s 70ms/step - loss: 0.2404 - accuracy: 1.0000 - val_loss: 0.4307 - val_accuracy: 0.9286\n",
            "Epoch 107/120\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.2365 - accuracy: 1.0000 - val_loss: 0.4283 - val_accuracy: 0.9286\n",
            "Epoch 108/120\n",
            "10/10 [==============================] - 1s 71ms/step - loss: 0.2342 - accuracy: 1.0000 - val_loss: 0.4242 - val_accuracy: 0.9286\n",
            "Epoch 109/120\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 0.2319 - accuracy: 1.0000 - val_loss: 0.4208 - val_accuracy: 0.9286\n",
            "Epoch 110/120\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.2298 - accuracy: 1.0000 - val_loss: 0.4165 - val_accuracy: 0.9286\n",
            "Epoch 111/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.2270 - accuracy: 1.0000 - val_loss: 0.4142 - val_accuracy: 0.9286\n",
            "Epoch 112/120\n",
            "10/10 [==============================] - 0s 47ms/step - loss: 0.2250 - accuracy: 1.0000 - val_loss: 0.4139 - val_accuracy: 0.9286\n",
            "Epoch 113/120\n",
            "10/10 [==============================] - 0s 41ms/step - loss: 0.2222 - accuracy: 1.0000 - val_loss: 0.4132 - val_accuracy: 0.9286\n",
            "Epoch 114/120\n",
            "10/10 [==============================] - 0s 41ms/step - loss: 0.2211 - accuracy: 1.0000 - val_loss: 0.4151 - val_accuracy: 0.9286\n",
            "Epoch 115/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.2202 - accuracy: 1.0000 - val_loss: 0.4078 - val_accuracy: 0.9286\n",
            "Epoch 116/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.2170 - accuracy: 1.0000 - val_loss: 0.4029 - val_accuracy: 0.9286\n",
            "Epoch 117/120\n",
            "10/10 [==============================] - 0s 40ms/step - loss: 0.2141 - accuracy: 1.0000 - val_loss: 0.4025 - val_accuracy: 0.9286\n",
            "Epoch 118/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.2116 - accuracy: 1.0000 - val_loss: 0.4016 - val_accuracy: 0.9286\n",
            "Epoch 119/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.2100 - accuracy: 1.0000 - val_loss: 0.3973 - val_accuracy: 0.9286\n",
            "Epoch 120/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.2084 - accuracy: 1.0000 - val_loss: 0.3955 - val_accuracy: 0.9286\n",
            "2/2 [==============================] - 2s 147ms/step\n",
            "Epoch 1/120\n",
            "10/10 [==============================] - 9s 432ms/step - loss: 1.8420 - accuracy: 0.1951 - val_loss: 1.6694 - val_accuracy: 0.5000\n",
            "Epoch 2/120\n",
            "10/10 [==============================] - 1s 80ms/step - loss: 1.6136 - accuracy: 0.5122 - val_loss: 1.5414 - val_accuracy: 0.5714\n",
            "Epoch 3/120\n",
            "10/10 [==============================] - 1s 59ms/step - loss: 1.4824 - accuracy: 0.4390 - val_loss: 1.4626 - val_accuracy: 0.3571\n",
            "Epoch 4/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 1.3950 - accuracy: 0.4146 - val_loss: 1.4234 - val_accuracy: 0.3571\n",
            "Epoch 5/120\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 1.3248 - accuracy: 0.4878 - val_loss: 1.3877 - val_accuracy: 0.3571\n",
            "Epoch 6/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 1.2749 - accuracy: 0.5366 - val_loss: 1.3604 - val_accuracy: 0.4286\n",
            "Epoch 7/120\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 1.2333 - accuracy: 0.5610 - val_loss: 1.3336 - val_accuracy: 0.6429\n",
            "Epoch 8/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 1.1954 - accuracy: 0.6585 - val_loss: 1.2968 - val_accuracy: 0.7143\n",
            "Epoch 9/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 1.1535 - accuracy: 0.6585 - val_loss: 1.2640 - val_accuracy: 0.7143\n",
            "Epoch 10/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 1.1127 - accuracy: 0.7561 - val_loss: 1.2355 - val_accuracy: 0.7143\n",
            "Epoch 11/120\n",
            "10/10 [==============================] - 1s 55ms/step - loss: 1.0795 - accuracy: 0.7561 - val_loss: 1.2121 - val_accuracy: 0.7143\n",
            "Epoch 12/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 1.0480 - accuracy: 0.7805 - val_loss: 1.1895 - val_accuracy: 0.7143\n",
            "Epoch 13/120\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 1.0218 - accuracy: 0.8537 - val_loss: 1.1689 - val_accuracy: 0.7143\n",
            "Epoch 14/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.9919 - accuracy: 0.8293 - val_loss: 1.1494 - val_accuracy: 0.7143\n",
            "Epoch 15/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.9669 - accuracy: 0.8049 - val_loss: 1.1275 - val_accuracy: 0.7857\n",
            "Epoch 16/120\n",
            "10/10 [==============================] - 1s 70ms/step - loss: 0.9467 - accuracy: 0.8293 - val_loss: 1.1033 - val_accuracy: 0.7143\n",
            "Epoch 17/120\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 0.9209 - accuracy: 0.8537 - val_loss: 1.0879 - val_accuracy: 0.7857\n",
            "Epoch 18/120\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.8971 - accuracy: 0.8293 - val_loss: 1.0754 - val_accuracy: 0.7857\n",
            "Epoch 19/120\n",
            "10/10 [==============================] - 1s 66ms/step - loss: 0.8765 - accuracy: 0.8293 - val_loss: 1.0634 - val_accuracy: 0.7857\n",
            "Epoch 20/120\n",
            "10/10 [==============================] - 1s 68ms/step - loss: 0.8593 - accuracy: 0.8293 - val_loss: 1.0472 - val_accuracy: 0.7857\n",
            "Epoch 21/120\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.8369 - accuracy: 0.8293 - val_loss: 1.0324 - val_accuracy: 0.7857\n",
            "Epoch 22/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.8150 - accuracy: 0.8293 - val_loss: 1.0239 - val_accuracy: 0.7857\n",
            "Epoch 23/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.7977 - accuracy: 0.8293 - val_loss: 1.0134 - val_accuracy: 0.7857\n",
            "Epoch 24/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.7823 - accuracy: 0.8293 - val_loss: 0.9994 - val_accuracy: 0.7857\n",
            "Epoch 25/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.7663 - accuracy: 0.8293 - val_loss: 0.9895 - val_accuracy: 0.7857\n",
            "Epoch 26/120\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 0.7502 - accuracy: 0.8293 - val_loss: 0.9732 - val_accuracy: 0.7857\n",
            "Epoch 27/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.7328 - accuracy: 0.8293 - val_loss: 0.9610 - val_accuracy: 0.7857\n",
            "Epoch 28/120\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.7189 - accuracy: 0.8537 - val_loss: 0.9483 - val_accuracy: 0.7857\n",
            "Epoch 29/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.7061 - accuracy: 0.9024 - val_loss: 0.9357 - val_accuracy: 0.7857\n",
            "Epoch 30/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.6930 - accuracy: 0.9024 - val_loss: 0.9270 - val_accuracy: 0.7857\n",
            "Epoch 31/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.6781 - accuracy: 0.9024 - val_loss: 0.9207 - val_accuracy: 0.7857\n",
            "Epoch 32/120\n",
            "10/10 [==============================] - 0s 46ms/step - loss: 0.6673 - accuracy: 0.9024 - val_loss: 0.9198 - val_accuracy: 0.7857\n",
            "Epoch 33/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.6548 - accuracy: 0.9268 - val_loss: 0.9047 - val_accuracy: 0.7857\n",
            "Epoch 34/120\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.6435 - accuracy: 0.9268 - val_loss: 0.8973 - val_accuracy: 0.7857\n",
            "Epoch 35/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.6311 - accuracy: 0.9268 - val_loss: 0.8851 - val_accuracy: 0.7857\n",
            "Epoch 36/120\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.6187 - accuracy: 0.9512 - val_loss: 0.8733 - val_accuracy: 0.7857\n",
            "Epoch 37/120\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.6081 - accuracy: 0.9512 - val_loss: 0.8673 - val_accuracy: 0.7857\n",
            "Epoch 38/120\n",
            "10/10 [==============================] - 1s 55ms/step - loss: 0.6038 - accuracy: 0.9512 - val_loss: 0.8483 - val_accuracy: 0.7857\n",
            "Epoch 39/120\n",
            "10/10 [==============================] - 1s 70ms/step - loss: 0.5885 - accuracy: 0.9512 - val_loss: 0.8377 - val_accuracy: 0.7857\n",
            "Epoch 40/120\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.5783 - accuracy: 0.9512 - val_loss: 0.8306 - val_accuracy: 0.7857\n",
            "Epoch 41/120\n",
            "10/10 [==============================] - 1s 70ms/step - loss: 0.5675 - accuracy: 0.9512 - val_loss: 0.8260 - val_accuracy: 0.7857\n",
            "Epoch 42/120\n",
            "10/10 [==============================] - 1s 66ms/step - loss: 0.5591 - accuracy: 0.9512 - val_loss: 0.8152 - val_accuracy: 0.7857\n",
            "Epoch 43/120\n",
            "10/10 [==============================] - 1s 71ms/step - loss: 0.5520 - accuracy: 0.9512 - val_loss: 0.8089 - val_accuracy: 0.7857\n",
            "Epoch 44/120\n",
            "10/10 [==============================] - 1s 84ms/step - loss: 0.5419 - accuracy: 0.9512 - val_loss: 0.7966 - val_accuracy: 0.7857\n",
            "Epoch 45/120\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 0.5348 - accuracy: 0.9512 - val_loss: 0.7856 - val_accuracy: 0.7857\n",
            "Epoch 46/120\n",
            "10/10 [==============================] - 1s 68ms/step - loss: 0.5253 - accuracy: 0.9512 - val_loss: 0.7777 - val_accuracy: 0.7857\n",
            "Epoch 47/120\n",
            "10/10 [==============================] - 1s 70ms/step - loss: 0.5151 - accuracy: 0.9512 - val_loss: 0.7718 - val_accuracy: 0.7857\n",
            "Epoch 48/120\n",
            "10/10 [==============================] - 0s 49ms/step - loss: 0.5059 - accuracy: 0.9512 - val_loss: 0.7714 - val_accuracy: 0.7857\n",
            "Epoch 49/120\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.5012 - accuracy: 0.9512 - val_loss: 0.7688 - val_accuracy: 0.8571\n",
            "Epoch 50/120\n",
            "10/10 [==============================] - 1s 68ms/step - loss: 0.4938 - accuracy: 0.9512 - val_loss: 0.7633 - val_accuracy: 0.8571\n",
            "Epoch 51/120\n",
            "10/10 [==============================] - 1s 60ms/step - loss: 0.4892 - accuracy: 0.9512 - val_loss: 0.7609 - val_accuracy: 0.8571\n",
            "Epoch 52/120\n",
            "10/10 [==============================] - 1s 55ms/step - loss: 0.4794 - accuracy: 0.9512 - val_loss: 0.7588 - val_accuracy: 0.7857\n",
            "Epoch 53/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.4714 - accuracy: 0.9512 - val_loss: 0.7574 - val_accuracy: 0.8571\n",
            "Epoch 54/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.4650 - accuracy: 0.9512 - val_loss: 0.7508 - val_accuracy: 0.8571\n",
            "Epoch 55/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.4570 - accuracy: 0.9512 - val_loss: 0.7443 - val_accuracy: 0.8571\n",
            "Epoch 56/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.4511 - accuracy: 0.9512 - val_loss: 0.7381 - val_accuracy: 0.7857\n",
            "Epoch 57/120\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.4457 - accuracy: 0.9512 - val_loss: 0.7313 - val_accuracy: 0.7857\n",
            "Epoch 58/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.4395 - accuracy: 0.9512 - val_loss: 0.7207 - val_accuracy: 0.8571\n",
            "Epoch 59/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.4333 - accuracy: 0.9512 - val_loss: 0.7141 - val_accuracy: 0.7857\n",
            "Epoch 60/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.4264 - accuracy: 0.9512 - val_loss: 0.7088 - val_accuracy: 0.7857\n",
            "Epoch 61/120\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 0.4201 - accuracy: 0.9512 - val_loss: 0.7055 - val_accuracy: 0.8571\n",
            "Epoch 62/120\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.4148 - accuracy: 0.9512 - val_loss: 0.7020 - val_accuracy: 0.8571\n",
            "Epoch 63/120\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.4109 - accuracy: 0.9512 - val_loss: 0.6989 - val_accuracy: 0.8571\n",
            "Epoch 64/120\n",
            "10/10 [==============================] - 1s 68ms/step - loss: 0.4044 - accuracy: 0.9512 - val_loss: 0.6937 - val_accuracy: 0.8571\n",
            "Epoch 65/120\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.3983 - accuracy: 0.9512 - val_loss: 0.6923 - val_accuracy: 0.8571\n",
            "Epoch 66/120\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 0.3936 - accuracy: 0.9512 - val_loss: 0.6901 - val_accuracy: 0.8571\n",
            "Epoch 67/120\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.3869 - accuracy: 0.9512 - val_loss: 0.6853 - val_accuracy: 0.8571\n",
            "Epoch 68/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.3842 - accuracy: 0.9756 - val_loss: 0.6716 - val_accuracy: 0.8571\n",
            "Epoch 69/120\n",
            "10/10 [==============================] - 1s 55ms/step - loss: 0.3773 - accuracy: 0.9756 - val_loss: 0.6650 - val_accuracy: 0.8571\n",
            "Epoch 70/120\n",
            "10/10 [==============================] - 0s 48ms/step - loss: 0.3727 - accuracy: 0.9756 - val_loss: 0.6641 - val_accuracy: 0.8571\n",
            "Epoch 71/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.3665 - accuracy: 0.9756 - val_loss: 0.6578 - val_accuracy: 0.8571\n",
            "Epoch 72/120\n",
            "10/10 [==============================] - 1s 58ms/step - loss: 0.3630 - accuracy: 0.9756 - val_loss: 0.6555 - val_accuracy: 0.8571\n",
            "Epoch 73/120\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.3571 - accuracy: 0.9756 - val_loss: 0.6525 - val_accuracy: 0.8571\n",
            "Epoch 74/120\n",
            "10/10 [==============================] - 1s 58ms/step - loss: 0.3540 - accuracy: 0.9756 - val_loss: 0.6507 - val_accuracy: 0.8571\n",
            "Epoch 75/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.3495 - accuracy: 0.9756 - val_loss: 0.6490 - val_accuracy: 0.8571\n",
            "Epoch 76/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.3443 - accuracy: 0.9756 - val_loss: 0.6475 - val_accuracy: 0.8571\n",
            "Epoch 77/120\n",
            "10/10 [==============================] - 0s 40ms/step - loss: 0.3409 - accuracy: 0.9756 - val_loss: 0.6467 - val_accuracy: 0.8571\n",
            "Epoch 78/120\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 0.3366 - accuracy: 0.9756 - val_loss: 0.6428 - val_accuracy: 0.8571\n",
            "Epoch 79/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.3355 - accuracy: 0.9756 - val_loss: 0.6345 - val_accuracy: 0.8571\n",
            "Epoch 80/120\n",
            "10/10 [==============================] - 0s 46ms/step - loss: 0.3285 - accuracy: 0.9756 - val_loss: 0.6356 - val_accuracy: 0.8571\n",
            "Epoch 81/120\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.3252 - accuracy: 0.9756 - val_loss: 0.6245 - val_accuracy: 0.8571\n",
            "Epoch 82/120\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.3211 - accuracy: 0.9756 - val_loss: 0.6188 - val_accuracy: 0.8571\n",
            "Epoch 83/120\n",
            "10/10 [==============================] - 1s 59ms/step - loss: 0.3176 - accuracy: 0.9756 - val_loss: 0.6146 - val_accuracy: 0.8571\n",
            "Epoch 84/120\n",
            "10/10 [==============================] - 0s 46ms/step - loss: 0.3168 - accuracy: 0.9756 - val_loss: 0.6183 - val_accuracy: 0.8571\n",
            "Epoch 85/120\n",
            "10/10 [==============================] - 0s 44ms/step - loss: 0.3119 - accuracy: 0.9756 - val_loss: 0.6154 - val_accuracy: 0.8571\n",
            "Epoch 86/120\n",
            "10/10 [==============================] - 1s 68ms/step - loss: 0.3056 - accuracy: 0.9756 - val_loss: 0.6104 - val_accuracy: 0.8571\n",
            "Epoch 87/120\n",
            "10/10 [==============================] - 1s 70ms/step - loss: 0.3016 - accuracy: 0.9756 - val_loss: 0.6058 - val_accuracy: 0.9286\n",
            "Epoch 88/120\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.2991 - accuracy: 0.9756 - val_loss: 0.6016 - val_accuracy: 0.9286\n",
            "Epoch 89/120\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 0.2966 - accuracy: 0.9756 - val_loss: 0.6004 - val_accuracy: 0.9286\n",
            "Epoch 90/120\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 0.2926 - accuracy: 0.9756 - val_loss: 0.5977 - val_accuracy: 0.9286\n",
            "Epoch 91/120\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 0.2889 - accuracy: 0.9756 - val_loss: 0.5884 - val_accuracy: 0.9286\n",
            "Epoch 92/120\n",
            "10/10 [==============================] - 1s 55ms/step - loss: 0.2857 - accuracy: 0.9756 - val_loss: 0.5863 - val_accuracy: 0.9286\n",
            "Epoch 93/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.2811 - accuracy: 0.9756 - val_loss: 0.5850 - val_accuracy: 0.9286\n",
            "Epoch 94/120\n",
            "10/10 [==============================] - 0s 47ms/step - loss: 0.2796 - accuracy: 0.9756 - val_loss: 0.5873 - val_accuracy: 0.8571\n",
            "Epoch 95/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.2737 - accuracy: 0.9756 - val_loss: 0.5830 - val_accuracy: 0.9286\n",
            "Epoch 96/120\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 0.2712 - accuracy: 0.9756 - val_loss: 0.5801 - val_accuracy: 0.9286\n",
            "Epoch 97/120\n",
            "10/10 [==============================] - 0s 40ms/step - loss: 0.2693 - accuracy: 0.9756 - val_loss: 0.5824 - val_accuracy: 0.9286\n",
            "Epoch 98/120\n",
            "10/10 [==============================] - 0s 40ms/step - loss: 0.2650 - accuracy: 0.9756 - val_loss: 0.5793 - val_accuracy: 0.9286\n",
            "Epoch 99/120\n",
            "10/10 [==============================] - 1s 55ms/step - loss: 0.2618 - accuracy: 0.9756 - val_loss: 0.5785 - val_accuracy: 0.9286\n",
            "Epoch 100/120\n",
            "10/10 [==============================] - 0s 41ms/step - loss: 0.2600 - accuracy: 0.9756 - val_loss: 0.5775 - val_accuracy: 0.7857\n",
            "Epoch 101/120\n",
            "10/10 [==============================] - 0s 47ms/step - loss: 0.2561 - accuracy: 0.9756 - val_loss: 0.5793 - val_accuracy: 0.7857\n",
            "Epoch 102/120\n",
            "10/10 [==============================] - 1s 55ms/step - loss: 0.2548 - accuracy: 0.9756 - val_loss: 0.5766 - val_accuracy: 0.7857\n",
            "Epoch 103/120\n",
            "10/10 [==============================] - 1s 58ms/step - loss: 0.2533 - accuracy: 0.9756 - val_loss: 0.5754 - val_accuracy: 0.7857\n",
            "Epoch 104/120\n",
            "10/10 [==============================] - 1s 59ms/step - loss: 0.2483 - accuracy: 0.9756 - val_loss: 0.5743 - val_accuracy: 0.7857\n",
            "Epoch 105/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.2457 - accuracy: 0.9756 - val_loss: 0.5727 - val_accuracy: 0.7857\n",
            "Epoch 106/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.2442 - accuracy: 0.9756 - val_loss: 0.5701 - val_accuracy: 0.7857\n",
            "Epoch 107/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.2409 - accuracy: 0.9756 - val_loss: 0.5669 - val_accuracy: 0.7857\n",
            "Epoch 108/120\n",
            "10/10 [==============================] - 0s 40ms/step - loss: 0.2383 - accuracy: 0.9756 - val_loss: 0.5679 - val_accuracy: 0.8571\n",
            "Epoch 109/120\n",
            "10/10 [==============================] - 1s 58ms/step - loss: 0.2347 - accuracy: 0.9756 - val_loss: 0.5653 - val_accuracy: 0.9286\n",
            "Epoch 110/120\n",
            "10/10 [==============================] - 0s 48ms/step - loss: 0.2331 - accuracy: 0.9756 - val_loss: 0.5658 - val_accuracy: 0.7857\n",
            "Epoch 111/120\n",
            "10/10 [==============================] - 0s 49ms/step - loss: 0.2312 - accuracy: 0.9756 - val_loss: 0.5645 - val_accuracy: 0.7857\n",
            "Epoch 112/120\n",
            "10/10 [==============================] - 1s 68ms/step - loss: 0.2279 - accuracy: 0.9756 - val_loss: 0.5631 - val_accuracy: 0.7857\n",
            "Epoch 113/120\n",
            "10/10 [==============================] - 1s 52ms/step - loss: 0.2263 - accuracy: 0.9756 - val_loss: 0.5638 - val_accuracy: 0.7857\n",
            "Epoch 114/120\n",
            "10/10 [==============================] - 1s 68ms/step - loss: 0.2236 - accuracy: 0.9756 - val_loss: 0.5592 - val_accuracy: 0.7857\n",
            "Epoch 115/120\n",
            "10/10 [==============================] - 1s 70ms/step - loss: 0.2204 - accuracy: 0.9756 - val_loss: 0.5534 - val_accuracy: 0.8571\n",
            "Epoch 116/120\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 0.2190 - accuracy: 0.9756 - val_loss: 0.5518 - val_accuracy: 0.8571\n",
            "Epoch 117/120\n",
            "10/10 [==============================] - 1s 58ms/step - loss: 0.2167 - accuracy: 0.9756 - val_loss: 0.5494 - val_accuracy: 0.9286\n",
            "Epoch 118/120\n",
            "10/10 [==============================] - 0s 42ms/step - loss: 0.2149 - accuracy: 0.9756 - val_loss: 0.5492 - val_accuracy: 0.9286\n",
            "Epoch 119/120\n",
            "10/10 [==============================] - 0s 46ms/step - loss: 0.2124 - accuracy: 0.9756 - val_loss: 0.5500 - val_accuracy: 0.8571\n",
            "Epoch 120/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.2102 - accuracy: 0.9756 - val_loss: 0.5483 - val_accuracy: 0.7857\n",
            "2/2 [==============================] - 2s 161ms/step\n",
            "Epoch 1/120\n",
            "10/10 [==============================] - 6s 237ms/step - loss: 1.6550 - accuracy: 0.0976 - val_loss: 1.5929 - val_accuracy: 0.2143\n",
            "Epoch 2/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 1.4838 - accuracy: 0.3902 - val_loss: 1.5073 - val_accuracy: 0.4286\n",
            "Epoch 3/120\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 1.3804 - accuracy: 0.5122 - val_loss: 1.4307 - val_accuracy: 0.3571\n",
            "Epoch 4/120\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 1.2951 - accuracy: 0.4878 - val_loss: 1.3745 - val_accuracy: 0.4286\n",
            "Epoch 5/120\n",
            "10/10 [==============================] - 1s 58ms/step - loss: 1.2287 - accuracy: 0.5854 - val_loss: 1.3233 - val_accuracy: 0.5000\n",
            "Epoch 6/120\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 1.1668 - accuracy: 0.6341 - val_loss: 1.2762 - val_accuracy: 0.5714\n",
            "Epoch 7/120\n",
            "10/10 [==============================] - 1s 77ms/step - loss: 1.1072 - accuracy: 0.7073 - val_loss: 1.2276 - val_accuracy: 0.7143\n",
            "Epoch 8/120\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 1.0576 - accuracy: 0.7073 - val_loss: 1.1786 - val_accuracy: 0.7143\n",
            "Epoch 9/120\n",
            "10/10 [==============================] - 1s 71ms/step - loss: 1.0098 - accuracy: 0.7317 - val_loss: 1.1369 - val_accuracy: 0.7857\n",
            "Epoch 10/120\n",
            "10/10 [==============================] - 1s 71ms/step - loss: 0.9727 - accuracy: 0.7561 - val_loss: 1.1022 - val_accuracy: 0.8571\n",
            "Epoch 11/120\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.9331 - accuracy: 0.7561 - val_loss: 1.0739 - val_accuracy: 0.8571\n",
            "Epoch 12/120\n",
            "10/10 [==============================] - 1s 71ms/step - loss: 0.9048 - accuracy: 0.8049 - val_loss: 1.0377 - val_accuracy: 0.7857\n",
            "Epoch 13/120\n",
            "10/10 [==============================] - 1s 58ms/step - loss: 0.8635 - accuracy: 0.8537 - val_loss: 1.0037 - val_accuracy: 0.7857\n",
            "Epoch 14/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.8346 - accuracy: 0.8537 - val_loss: 0.9766 - val_accuracy: 0.7857\n",
            "Epoch 15/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.8080 - accuracy: 0.8537 - val_loss: 0.9506 - val_accuracy: 0.7857\n",
            "Epoch 16/120\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.7823 - accuracy: 0.8780 - val_loss: 0.9292 - val_accuracy: 0.7857\n",
            "Epoch 17/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.7560 - accuracy: 0.9024 - val_loss: 0.9075 - val_accuracy: 0.7857\n",
            "Epoch 18/120\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.7379 - accuracy: 0.9268 - val_loss: 0.8857 - val_accuracy: 0.8571\n",
            "Epoch 19/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.7103 - accuracy: 0.8780 - val_loss: 0.8650 - val_accuracy: 0.8571\n",
            "Epoch 20/120\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 0.6904 - accuracy: 0.9024 - val_loss: 0.8479 - val_accuracy: 0.8571\n",
            "Epoch 21/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.6784 - accuracy: 0.9268 - val_loss: 0.8287 - val_accuracy: 0.8571\n",
            "Epoch 22/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.6531 - accuracy: 0.9268 - val_loss: 0.8072 - val_accuracy: 0.7857\n",
            "Epoch 23/120\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 0.6332 - accuracy: 0.9512 - val_loss: 0.7935 - val_accuracy: 0.8571\n",
            "Epoch 24/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.6185 - accuracy: 0.9512 - val_loss: 0.7757 - val_accuracy: 0.9286\n",
            "Epoch 25/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.6016 - accuracy: 0.9268 - val_loss: 0.7630 - val_accuracy: 0.9286\n",
            "Epoch 26/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.5870 - accuracy: 0.9512 - val_loss: 0.7480 - val_accuracy: 0.9286\n",
            "Epoch 27/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.5739 - accuracy: 0.9268 - val_loss: 0.7283 - val_accuracy: 0.9286\n",
            "Epoch 28/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.5585 - accuracy: 0.9512 - val_loss: 0.7117 - val_accuracy: 0.9286\n",
            "Epoch 29/120\n",
            "10/10 [==============================] - 1s 66ms/step - loss: 0.5463 - accuracy: 0.9268 - val_loss: 0.7000 - val_accuracy: 0.9286\n",
            "Epoch 30/120\n",
            "10/10 [==============================] - 1s 73ms/step - loss: 0.5342 - accuracy: 0.9268 - val_loss: 0.6826 - val_accuracy: 0.9286\n",
            "Epoch 31/120\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 0.5227 - accuracy: 0.9024 - val_loss: 0.6717 - val_accuracy: 0.9286\n",
            "Epoch 32/120\n",
            "10/10 [==============================] - 1s 71ms/step - loss: 0.5122 - accuracy: 0.9268 - val_loss: 0.6586 - val_accuracy: 0.9286\n",
            "Epoch 33/120\n",
            "10/10 [==============================] - 1s 70ms/step - loss: 0.4994 - accuracy: 0.9268 - val_loss: 0.6469 - val_accuracy: 0.9286\n",
            "Epoch 34/120\n",
            "10/10 [==============================] - 1s 70ms/step - loss: 0.4853 - accuracy: 0.9268 - val_loss: 0.6393 - val_accuracy: 0.9286\n",
            "Epoch 35/120\n",
            "10/10 [==============================] - 1s 66ms/step - loss: 0.4772 - accuracy: 0.9024 - val_loss: 0.6313 - val_accuracy: 0.9286\n",
            "Epoch 36/120\n",
            "10/10 [==============================] - 1s 58ms/step - loss: 0.4675 - accuracy: 0.9268 - val_loss: 0.6157 - val_accuracy: 0.9286\n",
            "Epoch 37/120\n",
            "10/10 [==============================] - 1s 59ms/step - loss: 0.4659 - accuracy: 0.9268 - val_loss: 0.6086 - val_accuracy: 0.9286\n",
            "Epoch 38/120\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.4518 - accuracy: 0.9512 - val_loss: 0.5939 - val_accuracy: 0.9286\n",
            "Epoch 39/120\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.4381 - accuracy: 0.9512 - val_loss: 0.5867 - val_accuracy: 0.9286\n",
            "Epoch 40/120\n",
            "10/10 [==============================] - 1s 58ms/step - loss: 0.4281 - accuracy: 0.9512 - val_loss: 0.5788 - val_accuracy: 0.9286\n",
            "Epoch 41/120\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.4233 - accuracy: 0.9512 - val_loss: 0.5739 - val_accuracy: 0.9286\n",
            "Epoch 42/120\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.4113 - accuracy: 0.9512 - val_loss: 0.5659 - val_accuracy: 0.9286\n",
            "Epoch 43/120\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.4047 - accuracy: 0.9512 - val_loss: 0.5526 - val_accuracy: 0.9286\n",
            "Epoch 44/120\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.3975 - accuracy: 0.9512 - val_loss: 0.5453 - val_accuracy: 0.9286\n",
            "Epoch 45/120\n",
            "10/10 [==============================] - 1s 59ms/step - loss: 0.3873 - accuracy: 0.9756 - val_loss: 0.5441 - val_accuracy: 0.9286\n",
            "Epoch 46/120\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.3791 - accuracy: 0.9756 - val_loss: 0.5374 - val_accuracy: 0.9286\n",
            "Epoch 47/120\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.3767 - accuracy: 0.9756 - val_loss: 0.5282 - val_accuracy: 0.9286\n",
            "Epoch 48/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.3658 - accuracy: 0.9756 - val_loss: 0.5218 - val_accuracy: 0.9286\n",
            "Epoch 49/120\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.3606 - accuracy: 0.9756 - val_loss: 0.5104 - val_accuracy: 0.9286\n",
            "Epoch 50/120\n",
            "10/10 [==============================] - 1s 58ms/step - loss: 0.3530 - accuracy: 0.9756 - val_loss: 0.5046 - val_accuracy: 0.9286\n",
            "Epoch 51/120\n",
            "10/10 [==============================] - 1s 59ms/step - loss: 0.3480 - accuracy: 0.9756 - val_loss: 0.4961 - val_accuracy: 0.9286\n",
            "Epoch 52/120\n",
            "10/10 [==============================] - 1s 53ms/step - loss: 0.3395 - accuracy: 0.9756 - val_loss: 0.4981 - val_accuracy: 0.9286\n",
            "Epoch 53/120\n",
            "10/10 [==============================] - 1s 51ms/step - loss: 0.3367 - accuracy: 0.9756 - val_loss: 0.4992 - val_accuracy: 0.8571\n",
            "Epoch 54/120\n",
            "10/10 [==============================] - 1s 71ms/step - loss: 0.3323 - accuracy: 0.9756 - val_loss: 0.4898 - val_accuracy: 0.8571\n",
            "Epoch 55/120\n",
            "10/10 [==============================] - 1s 66ms/step - loss: 0.3240 - accuracy: 0.9756 - val_loss: 0.4815 - val_accuracy: 0.9286\n",
            "Epoch 56/120\n",
            "10/10 [==============================] - 1s 71ms/step - loss: 0.3188 - accuracy: 0.9756 - val_loss: 0.4739 - val_accuracy: 0.9286\n",
            "Epoch 57/120\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 0.3141 - accuracy: 0.9756 - val_loss: 0.4672 - val_accuracy: 0.9286\n",
            "Epoch 58/120\n",
            "10/10 [==============================] - 1s 68ms/step - loss: 0.3079 - accuracy: 0.9756 - val_loss: 0.4557 - val_accuracy: 0.9286\n",
            "Epoch 59/120\n",
            "10/10 [==============================] - 1s 58ms/step - loss: 0.3028 - accuracy: 0.9756 - val_loss: 0.4540 - val_accuracy: 0.9286\n",
            "Epoch 60/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.2990 - accuracy: 0.9756 - val_loss: 0.4493 - val_accuracy: 0.9286\n",
            "Epoch 61/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.2972 - accuracy: 0.9756 - val_loss: 0.4283 - val_accuracy: 0.9286\n",
            "Epoch 62/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.2897 - accuracy: 1.0000 - val_loss: 0.4246 - val_accuracy: 0.9286\n",
            "Epoch 63/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.2847 - accuracy: 1.0000 - val_loss: 0.4231 - val_accuracy: 0.9286\n",
            "Epoch 64/120\n",
            "10/10 [==============================] - 0s 49ms/step - loss: 0.2795 - accuracy: 1.0000 - val_loss: 0.4225 - val_accuracy: 0.9286\n",
            "Epoch 65/120\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.2746 - accuracy: 1.0000 - val_loss: 0.4210 - val_accuracy: 0.9286\n",
            "Epoch 66/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.2705 - accuracy: 1.0000 - val_loss: 0.4177 - val_accuracy: 0.9286\n",
            "Epoch 67/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.2666 - accuracy: 1.0000 - val_loss: 0.4159 - val_accuracy: 0.9286\n",
            "Epoch 68/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.2606 - accuracy: 1.0000 - val_loss: 0.4073 - val_accuracy: 0.9286\n",
            "Epoch 69/120\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.2588 - accuracy: 1.0000 - val_loss: 0.3993 - val_accuracy: 0.9286\n",
            "Epoch 70/120\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.2548 - accuracy: 1.0000 - val_loss: 0.3979 - val_accuracy: 0.9286\n",
            "Epoch 71/120\n",
            "10/10 [==============================] - 0s 48ms/step - loss: 0.2505 - accuracy: 1.0000 - val_loss: 0.3974 - val_accuracy: 0.9286\n",
            "Epoch 72/120\n",
            "10/10 [==============================] - 0s 41ms/step - loss: 0.2477 - accuracy: 1.0000 - val_loss: 0.3977 - val_accuracy: 0.9286\n",
            "Epoch 73/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.2442 - accuracy: 1.0000 - val_loss: 0.3942 - val_accuracy: 0.9286\n",
            "Epoch 74/120\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.2398 - accuracy: 1.0000 - val_loss: 0.3890 - val_accuracy: 0.9286\n",
            "Epoch 75/120\n",
            "10/10 [==============================] - 1s 60ms/step - loss: 0.2382 - accuracy: 1.0000 - val_loss: 0.3872 - val_accuracy: 0.9286\n",
            "Epoch 76/120\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 0.2345 - accuracy: 1.0000 - val_loss: 0.3831 - val_accuracy: 0.9286\n",
            "Epoch 77/120\n",
            "10/10 [==============================] - 1s 68ms/step - loss: 0.2309 - accuracy: 1.0000 - val_loss: 0.3789 - val_accuracy: 0.9286\n",
            "Epoch 78/120\n",
            "10/10 [==============================] - 1s 52ms/step - loss: 0.2284 - accuracy: 1.0000 - val_loss: 0.3837 - val_accuracy: 0.9286\n",
            "Epoch 79/120\n",
            "10/10 [==============================] - 1s 70ms/step - loss: 0.2272 - accuracy: 1.0000 - val_loss: 0.3677 - val_accuracy: 0.9286\n",
            "Epoch 80/120\n",
            "10/10 [==============================] - 1s 72ms/step - loss: 0.2207 - accuracy: 1.0000 - val_loss: 0.3608 - val_accuracy: 0.9286\n",
            "Epoch 81/120\n",
            "10/10 [==============================] - 1s 52ms/step - loss: 0.2181 - accuracy: 1.0000 - val_loss: 0.3598 - val_accuracy: 0.9286\n",
            "Epoch 82/120\n",
            "10/10 [==============================] - 1s 66ms/step - loss: 0.2147 - accuracy: 1.0000 - val_loss: 0.3571 - val_accuracy: 0.9286\n",
            "Epoch 83/120\n",
            "10/10 [==============================] - 0s 47ms/step - loss: 0.2118 - accuracy: 1.0000 - val_loss: 0.3562 - val_accuracy: 0.9286\n",
            "Epoch 84/120\n",
            "10/10 [==============================] - 1s 58ms/step - loss: 0.2101 - accuracy: 1.0000 - val_loss: 0.3470 - val_accuracy: 0.9286\n",
            "Epoch 85/120\n",
            "10/10 [==============================] - 0s 43ms/step - loss: 0.2062 - accuracy: 1.0000 - val_loss: 0.3505 - val_accuracy: 0.9286\n",
            "Epoch 86/120\n",
            "10/10 [==============================] - 0s 47ms/step - loss: 0.2039 - accuracy: 1.0000 - val_loss: 0.3525 - val_accuracy: 0.9286\n",
            "Epoch 87/120\n",
            "10/10 [==============================] - 0s 42ms/step - loss: 0.2006 - accuracy: 1.0000 - val_loss: 0.3520 - val_accuracy: 0.9286\n",
            "Epoch 88/120\n",
            "10/10 [==============================] - 0s 48ms/step - loss: 0.1991 - accuracy: 1.0000 - val_loss: 0.3477 - val_accuracy: 0.9286\n",
            "Epoch 89/120\n",
            " 9/10 [=========================>....] - ETA: 0s - loss: 0.2123 - accuracy: 1.0000Restoring model weights from the end of the best epoch: 84.\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 0.1954 - accuracy: 1.0000 - val_loss: 0.3468 - val_accuracy: 0.9286\n",
            "Epoch 89: early stopping\n",
            "2/2 [==============================] - 1s 115ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = []\n",
        "for i in range(5):\n",
        "  loss.append(histories[i].history[\"val_loss\"][-1])"
      ],
      "metadata": {
        "id": "3w7G2FGEn6kL"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsaBRtB3Ht19",
        "outputId": "3eeb423f-946b-4abd-f572-f1795188f897"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4015568196773529,\n",
              " 0.5032914876937866,\n",
              " 0.395497590303421,\n",
              " 0.5483052134513855,\n",
              " 0.3468170762062073]"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds_label_name = []\n",
        "for predict in predictions:\n",
        "  pred_label_name = []\n",
        "  for idx in predict:\n",
        "    pred_label_name.append(label_name[idx])\n",
        "  pred_label_name = np.array(pred_label_name)\n",
        "  preds_label_name.append(pred_label_name)"
      ],
      "metadata": {
        "id": "bdL6rKgaIGki"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_origin = []\n",
        "for i in range(5):\n",
        "  pred = np.argmax(models[i].predict(shrink_test_data), axis=1)\n",
        "  predictions_origin.append(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lvhu9iokFFLw",
        "outputId": "703b3f14-6c26-4ef8-dee4-07b46a0e7a59"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 159ms/step\n",
            "2/2 [==============================] - 0s 104ms/step\n",
            "2/2 [==============================] - 0s 110ms/step\n",
            "2/2 [==============================] - 0s 98ms/step\n",
            "2/2 [==============================] - 0s 106ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "origin_preds_label_name = []\n",
        "for predict in predictions_origin:\n",
        "  pred_label_name = []\n",
        "  for idx in predict:\n",
        "    pred_label_name.append(label_name[idx])\n",
        "  pred_label_name = np.array(pred_label_name)\n",
        "  origin_preds_label_name.append(pred_label_name)"
      ],
      "metadata": {
        "id": "Y_K5-nxmFmSZ"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct_label_name = []\n",
        "for idx in test_label:\n",
        "  correct_label_name.append(test_label_name[idx])"
      ],
      "metadata": {
        "id": "sGSkjZKbLJJM"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracies=[]\n",
        "for i in range(len(preds_label_name)):\n",
        "  accuracies.append(sum(preds_label_name[i] == correct_label_name))"
      ],
      "metadata": {
        "id": "kpQPrDAnLnu7"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "origin_accuracies=[]\n",
        "for i in range(len(origin_preds_label_name)):\n",
        "  origin_accuracies.append(sum(origin_preds_label_name[i] == correct_label_name))\n",
        "origin_accuracies"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZKaWXhNGEXQ",
        "outputId": "d5fda325-8005-44bc-dee0-7c825d398617"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[12, 12, 7, 7, 16]"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracies"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jdCJtEuK3OG",
        "outputId": "4e83631b-0201-4c20-e519-38670f9702ef"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[12, 12, 7, 7, 14]"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WxFBrcPdoFvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NDApred_label_name = []\n",
        "for idx in NDAprediction:\n",
        "  NDApred_label_name.append(label_name[idx])\n",
        "NDApred_label_name = np.array(NDApred_label_name)"
      ],
      "metadata": {
        "id": "DXsfsf9wccxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NDAaccuracy = sum(NDApred_label_name == correct_label_name)\n",
        "NDAaccuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_HyZtWtNyiM",
        "outputId": "462cde13-fde6-46db-c097-d09308880dcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models[6]\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66Oem6apTpOP",
        "outputId": "4e0cb809-fdef-4c07-928d-175fe036b277"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_7 (InputLayer)           [(None, 299, 299, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d_564 (Conv2D)            (None, 149, 149, 32  864         ['input_7[0][0]']                \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_564 (Batch  (None, 149, 149, 32  96         ['conv2d_564[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " activation_564 (Activation)    (None, 149, 149, 32  0           ['batch_normalization_564[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_565 (Conv2D)            (None, 147, 147, 32  9216        ['activation_564[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_565 (Batch  (None, 147, 147, 32  96         ['conv2d_565[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " activation_565 (Activation)    (None, 147, 147, 32  0           ['batch_normalization_565[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_566 (Conv2D)            (None, 147, 147, 64  18432       ['activation_565[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_566 (Batch  (None, 147, 147, 64  192        ['conv2d_566[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " activation_566 (Activation)    (None, 147, 147, 64  0           ['batch_normalization_566[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d_24 (MaxPooling2D  (None, 73, 73, 64)  0           ['activation_566[0][0]']         \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_567 (Conv2D)            (None, 73, 73, 80)   5120        ['max_pooling2d_24[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_567 (Batch  (None, 73, 73, 80)  240         ['conv2d_567[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_567 (Activation)    (None, 73, 73, 80)   0           ['batch_normalization_567[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_568 (Conv2D)            (None, 71, 71, 192)  138240      ['activation_567[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_568 (Batch  (None, 71, 71, 192)  576        ['conv2d_568[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_568 (Activation)    (None, 71, 71, 192)  0           ['batch_normalization_568[0][0]']\n",
            "                                                                                                  \n",
            " max_pooling2d_25 (MaxPooling2D  (None, 35, 35, 192)  0          ['activation_568[0][0]']         \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_572 (Conv2D)            (None, 35, 35, 64)   12288       ['max_pooling2d_25[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_572 (Batch  (None, 35, 35, 64)  192         ['conv2d_572[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_572 (Activation)    (None, 35, 35, 64)   0           ['batch_normalization_572[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_570 (Conv2D)            (None, 35, 35, 48)   9216        ['max_pooling2d_25[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_573 (Conv2D)            (None, 35, 35, 96)   55296       ['activation_572[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_570 (Batch  (None, 35, 35, 48)  144         ['conv2d_570[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_573 (Batch  (None, 35, 35, 96)  288         ['conv2d_573[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_570 (Activation)    (None, 35, 35, 48)   0           ['batch_normalization_570[0][0]']\n",
            "                                                                                                  \n",
            " activation_573 (Activation)    (None, 35, 35, 96)   0           ['batch_normalization_573[0][0]']\n",
            "                                                                                                  \n",
            " average_pooling2d_54 (AverageP  (None, 35, 35, 192)  0          ['max_pooling2d_25[0][0]']       \n",
            " ooling2D)                                                                                        \n",
            "                                                                                                  \n",
            " conv2d_569 (Conv2D)            (None, 35, 35, 64)   12288       ['max_pooling2d_25[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_571 (Conv2D)            (None, 35, 35, 64)   76800       ['activation_570[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_574 (Conv2D)            (None, 35, 35, 96)   82944       ['activation_573[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_575 (Conv2D)            (None, 35, 35, 32)   6144        ['average_pooling2d_54[0][0]']   \n",
            "                                                                                                  \n",
            " batch_normalization_569 (Batch  (None, 35, 35, 64)  192         ['conv2d_569[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_571 (Batch  (None, 35, 35, 64)  192         ['conv2d_571[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_574 (Batch  (None, 35, 35, 96)  288         ['conv2d_574[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_575 (Batch  (None, 35, 35, 32)  96          ['conv2d_575[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_569 (Activation)    (None, 35, 35, 64)   0           ['batch_normalization_569[0][0]']\n",
            "                                                                                                  \n",
            " activation_571 (Activation)    (None, 35, 35, 64)   0           ['batch_normalization_571[0][0]']\n",
            "                                                                                                  \n",
            " activation_574 (Activation)    (None, 35, 35, 96)   0           ['batch_normalization_574[0][0]']\n",
            "                                                                                                  \n",
            " activation_575 (Activation)    (None, 35, 35, 32)   0           ['batch_normalization_575[0][0]']\n",
            "                                                                                                  \n",
            " mixed0 (Concatenate)           (None, 35, 35, 256)  0           ['activation_569[0][0]',         \n",
            "                                                                  'activation_571[0][0]',         \n",
            "                                                                  'activation_574[0][0]',         \n",
            "                                                                  'activation_575[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_579 (Conv2D)            (None, 35, 35, 64)   16384       ['mixed0[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_579 (Batch  (None, 35, 35, 64)  192         ['conv2d_579[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_579 (Activation)    (None, 35, 35, 64)   0           ['batch_normalization_579[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_577 (Conv2D)            (None, 35, 35, 48)   12288       ['mixed0[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_580 (Conv2D)            (None, 35, 35, 96)   55296       ['activation_579[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_577 (Batch  (None, 35, 35, 48)  144         ['conv2d_577[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_580 (Batch  (None, 35, 35, 96)  288         ['conv2d_580[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_577 (Activation)    (None, 35, 35, 48)   0           ['batch_normalization_577[0][0]']\n",
            "                                                                                                  \n",
            " activation_580 (Activation)    (None, 35, 35, 96)   0           ['batch_normalization_580[0][0]']\n",
            "                                                                                                  \n",
            " average_pooling2d_55 (AverageP  (None, 35, 35, 256)  0          ['mixed0[0][0]']                 \n",
            " ooling2D)                                                                                        \n",
            "                                                                                                  \n",
            " conv2d_576 (Conv2D)            (None, 35, 35, 64)   16384       ['mixed0[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_578 (Conv2D)            (None, 35, 35, 64)   76800       ['activation_577[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_581 (Conv2D)            (None, 35, 35, 96)   82944       ['activation_580[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_582 (Conv2D)            (None, 35, 35, 64)   16384       ['average_pooling2d_55[0][0]']   \n",
            "                                                                                                  \n",
            " batch_normalization_576 (Batch  (None, 35, 35, 64)  192         ['conv2d_576[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_578 (Batch  (None, 35, 35, 64)  192         ['conv2d_578[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_581 (Batch  (None, 35, 35, 96)  288         ['conv2d_581[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_582 (Batch  (None, 35, 35, 64)  192         ['conv2d_582[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_576 (Activation)    (None, 35, 35, 64)   0           ['batch_normalization_576[0][0]']\n",
            "                                                                                                  \n",
            " activation_578 (Activation)    (None, 35, 35, 64)   0           ['batch_normalization_578[0][0]']\n",
            "                                                                                                  \n",
            " activation_581 (Activation)    (None, 35, 35, 96)   0           ['batch_normalization_581[0][0]']\n",
            "                                                                                                  \n",
            " activation_582 (Activation)    (None, 35, 35, 64)   0           ['batch_normalization_582[0][0]']\n",
            "                                                                                                  \n",
            " mixed1 (Concatenate)           (None, 35, 35, 288)  0           ['activation_576[0][0]',         \n",
            "                                                                  'activation_578[0][0]',         \n",
            "                                                                  'activation_581[0][0]',         \n",
            "                                                                  'activation_582[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_586 (Conv2D)            (None, 35, 35, 64)   18432       ['mixed1[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_586 (Batch  (None, 35, 35, 64)  192         ['conv2d_586[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_586 (Activation)    (None, 35, 35, 64)   0           ['batch_normalization_586[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_584 (Conv2D)            (None, 35, 35, 48)   13824       ['mixed1[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_587 (Conv2D)            (None, 35, 35, 96)   55296       ['activation_586[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_584 (Batch  (None, 35, 35, 48)  144         ['conv2d_584[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_587 (Batch  (None, 35, 35, 96)  288         ['conv2d_587[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_584 (Activation)    (None, 35, 35, 48)   0           ['batch_normalization_584[0][0]']\n",
            "                                                                                                  \n",
            " activation_587 (Activation)    (None, 35, 35, 96)   0           ['batch_normalization_587[0][0]']\n",
            "                                                                                                  \n",
            " average_pooling2d_56 (AverageP  (None, 35, 35, 288)  0          ['mixed1[0][0]']                 \n",
            " ooling2D)                                                                                        \n",
            "                                                                                                  \n",
            " conv2d_583 (Conv2D)            (None, 35, 35, 64)   18432       ['mixed1[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_585 (Conv2D)            (None, 35, 35, 64)   76800       ['activation_584[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_588 (Conv2D)            (None, 35, 35, 96)   82944       ['activation_587[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_589 (Conv2D)            (None, 35, 35, 64)   18432       ['average_pooling2d_56[0][0]']   \n",
            "                                                                                                  \n",
            " batch_normalization_583 (Batch  (None, 35, 35, 64)  192         ['conv2d_583[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_585 (Batch  (None, 35, 35, 64)  192         ['conv2d_585[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_588 (Batch  (None, 35, 35, 96)  288         ['conv2d_588[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_589 (Batch  (None, 35, 35, 64)  192         ['conv2d_589[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_583 (Activation)    (None, 35, 35, 64)   0           ['batch_normalization_583[0][0]']\n",
            "                                                                                                  \n",
            " activation_585 (Activation)    (None, 35, 35, 64)   0           ['batch_normalization_585[0][0]']\n",
            "                                                                                                  \n",
            " activation_588 (Activation)    (None, 35, 35, 96)   0           ['batch_normalization_588[0][0]']\n",
            "                                                                                                  \n",
            " activation_589 (Activation)    (None, 35, 35, 64)   0           ['batch_normalization_589[0][0]']\n",
            "                                                                                                  \n",
            " mixed2 (Concatenate)           (None, 35, 35, 288)  0           ['activation_583[0][0]',         \n",
            "                                                                  'activation_585[0][0]',         \n",
            "                                                                  'activation_588[0][0]',         \n",
            "                                                                  'activation_589[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_591 (Conv2D)            (None, 35, 35, 64)   18432       ['mixed2[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_591 (Batch  (None, 35, 35, 64)  192         ['conv2d_591[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_591 (Activation)    (None, 35, 35, 64)   0           ['batch_normalization_591[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_592 (Conv2D)            (None, 35, 35, 96)   55296       ['activation_591[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_592 (Batch  (None, 35, 35, 96)  288         ['conv2d_592[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_592 (Activation)    (None, 35, 35, 96)   0           ['batch_normalization_592[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_590 (Conv2D)            (None, 17, 17, 384)  995328      ['mixed2[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_593 (Conv2D)            (None, 17, 17, 96)   82944       ['activation_592[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_590 (Batch  (None, 17, 17, 384)  1152       ['conv2d_590[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_593 (Batch  (None, 17, 17, 96)  288         ['conv2d_593[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_590 (Activation)    (None, 17, 17, 384)  0           ['batch_normalization_590[0][0]']\n",
            "                                                                                                  \n",
            " activation_593 (Activation)    (None, 17, 17, 96)   0           ['batch_normalization_593[0][0]']\n",
            "                                                                                                  \n",
            " max_pooling2d_26 (MaxPooling2D  (None, 17, 17, 288)  0          ['mixed2[0][0]']                 \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " mixed3 (Concatenate)           (None, 17, 17, 768)  0           ['activation_590[0][0]',         \n",
            "                                                                  'activation_593[0][0]',         \n",
            "                                                                  'max_pooling2d_26[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_598 (Conv2D)            (None, 17, 17, 128)  98304       ['mixed3[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_598 (Batch  (None, 17, 17, 128)  384        ['conv2d_598[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_598 (Activation)    (None, 17, 17, 128)  0           ['batch_normalization_598[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_599 (Conv2D)            (None, 17, 17, 128)  114688      ['activation_598[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_599 (Batch  (None, 17, 17, 128)  384        ['conv2d_599[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_599 (Activation)    (None, 17, 17, 128)  0           ['batch_normalization_599[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_595 (Conv2D)            (None, 17, 17, 128)  98304       ['mixed3[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_600 (Conv2D)            (None, 17, 17, 128)  114688      ['activation_599[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_595 (Batch  (None, 17, 17, 128)  384        ['conv2d_595[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_600 (Batch  (None, 17, 17, 128)  384        ['conv2d_600[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_595 (Activation)    (None, 17, 17, 128)  0           ['batch_normalization_595[0][0]']\n",
            "                                                                                                  \n",
            " activation_600 (Activation)    (None, 17, 17, 128)  0           ['batch_normalization_600[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_596 (Conv2D)            (None, 17, 17, 128)  114688      ['activation_595[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_601 (Conv2D)            (None, 17, 17, 128)  114688      ['activation_600[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_596 (Batch  (None, 17, 17, 128)  384        ['conv2d_596[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_601 (Batch  (None, 17, 17, 128)  384        ['conv2d_601[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_596 (Activation)    (None, 17, 17, 128)  0           ['batch_normalization_596[0][0]']\n",
            "                                                                                                  \n",
            " activation_601 (Activation)    (None, 17, 17, 128)  0           ['batch_normalization_601[0][0]']\n",
            "                                                                                                  \n",
            " average_pooling2d_57 (AverageP  (None, 17, 17, 768)  0          ['mixed3[0][0]']                 \n",
            " ooling2D)                                                                                        \n",
            "                                                                                                  \n",
            " conv2d_594 (Conv2D)            (None, 17, 17, 192)  147456      ['mixed3[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_597 (Conv2D)            (None, 17, 17, 192)  172032      ['activation_596[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_602 (Conv2D)            (None, 17, 17, 192)  172032      ['activation_601[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_603 (Conv2D)            (None, 17, 17, 192)  147456      ['average_pooling2d_57[0][0]']   \n",
            "                                                                                                  \n",
            " batch_normalization_594 (Batch  (None, 17, 17, 192)  576        ['conv2d_594[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_597 (Batch  (None, 17, 17, 192)  576        ['conv2d_597[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_602 (Batch  (None, 17, 17, 192)  576        ['conv2d_602[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_603 (Batch  (None, 17, 17, 192)  576        ['conv2d_603[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_594 (Activation)    (None, 17, 17, 192)  0           ['batch_normalization_594[0][0]']\n",
            "                                                                                                  \n",
            " activation_597 (Activation)    (None, 17, 17, 192)  0           ['batch_normalization_597[0][0]']\n",
            "                                                                                                  \n",
            " activation_602 (Activation)    (None, 17, 17, 192)  0           ['batch_normalization_602[0][0]']\n",
            "                                                                                                  \n",
            " activation_603 (Activation)    (None, 17, 17, 192)  0           ['batch_normalization_603[0][0]']\n",
            "                                                                                                  \n",
            " mixed4 (Concatenate)           (None, 17, 17, 768)  0           ['activation_594[0][0]',         \n",
            "                                                                  'activation_597[0][0]',         \n",
            "                                                                  'activation_602[0][0]',         \n",
            "                                                                  'activation_603[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_608 (Conv2D)            (None, 17, 17, 160)  122880      ['mixed4[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_608 (Batch  (None, 17, 17, 160)  480        ['conv2d_608[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_608 (Activation)    (None, 17, 17, 160)  0           ['batch_normalization_608[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_609 (Conv2D)            (None, 17, 17, 160)  179200      ['activation_608[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_609 (Batch  (None, 17, 17, 160)  480        ['conv2d_609[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_609 (Activation)    (None, 17, 17, 160)  0           ['batch_normalization_609[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_605 (Conv2D)            (None, 17, 17, 160)  122880      ['mixed4[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_610 (Conv2D)            (None, 17, 17, 160)  179200      ['activation_609[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_605 (Batch  (None, 17, 17, 160)  480        ['conv2d_605[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_610 (Batch  (None, 17, 17, 160)  480        ['conv2d_610[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_605 (Activation)    (None, 17, 17, 160)  0           ['batch_normalization_605[0][0]']\n",
            "                                                                                                  \n",
            " activation_610 (Activation)    (None, 17, 17, 160)  0           ['batch_normalization_610[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_606 (Conv2D)            (None, 17, 17, 160)  179200      ['activation_605[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_611 (Conv2D)            (None, 17, 17, 160)  179200      ['activation_610[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_606 (Batch  (None, 17, 17, 160)  480        ['conv2d_606[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_611 (Batch  (None, 17, 17, 160)  480        ['conv2d_611[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_606 (Activation)    (None, 17, 17, 160)  0           ['batch_normalization_606[0][0]']\n",
            "                                                                                                  \n",
            " activation_611 (Activation)    (None, 17, 17, 160)  0           ['batch_normalization_611[0][0]']\n",
            "                                                                                                  \n",
            " average_pooling2d_58 (AverageP  (None, 17, 17, 768)  0          ['mixed4[0][0]']                 \n",
            " ooling2D)                                                                                        \n",
            "                                                                                                  \n",
            " conv2d_604 (Conv2D)            (None, 17, 17, 192)  147456      ['mixed4[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_607 (Conv2D)            (None, 17, 17, 192)  215040      ['activation_606[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_612 (Conv2D)            (None, 17, 17, 192)  215040      ['activation_611[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_613 (Conv2D)            (None, 17, 17, 192)  147456      ['average_pooling2d_58[0][0]']   \n",
            "                                                                                                  \n",
            " batch_normalization_604 (Batch  (None, 17, 17, 192)  576        ['conv2d_604[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_607 (Batch  (None, 17, 17, 192)  576        ['conv2d_607[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_612 (Batch  (None, 17, 17, 192)  576        ['conv2d_612[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_613 (Batch  (None, 17, 17, 192)  576        ['conv2d_613[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_604 (Activation)    (None, 17, 17, 192)  0           ['batch_normalization_604[0][0]']\n",
            "                                                                                                  \n",
            " activation_607 (Activation)    (None, 17, 17, 192)  0           ['batch_normalization_607[0][0]']\n",
            "                                                                                                  \n",
            " activation_612 (Activation)    (None, 17, 17, 192)  0           ['batch_normalization_612[0][0]']\n",
            "                                                                                                  \n",
            " activation_613 (Activation)    (None, 17, 17, 192)  0           ['batch_normalization_613[0][0]']\n",
            "                                                                                                  \n",
            " mixed5 (Concatenate)           (None, 17, 17, 768)  0           ['activation_604[0][0]',         \n",
            "                                                                  'activation_607[0][0]',         \n",
            "                                                                  'activation_612[0][0]',         \n",
            "                                                                  'activation_613[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_618 (Conv2D)            (None, 17, 17, 160)  122880      ['mixed5[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_618 (Batch  (None, 17, 17, 160)  480        ['conv2d_618[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_618 (Activation)    (None, 17, 17, 160)  0           ['batch_normalization_618[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_619 (Conv2D)            (None, 17, 17, 160)  179200      ['activation_618[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_619 (Batch  (None, 17, 17, 160)  480        ['conv2d_619[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_619 (Activation)    (None, 17, 17, 160)  0           ['batch_normalization_619[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_615 (Conv2D)            (None, 17, 17, 160)  122880      ['mixed5[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_620 (Conv2D)            (None, 17, 17, 160)  179200      ['activation_619[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_615 (Batch  (None, 17, 17, 160)  480        ['conv2d_615[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_620 (Batch  (None, 17, 17, 160)  480        ['conv2d_620[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_615 (Activation)    (None, 17, 17, 160)  0           ['batch_normalization_615[0][0]']\n",
            "                                                                                                  \n",
            " activation_620 (Activation)    (None, 17, 17, 160)  0           ['batch_normalization_620[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_616 (Conv2D)            (None, 17, 17, 160)  179200      ['activation_615[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_621 (Conv2D)            (None, 17, 17, 160)  179200      ['activation_620[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_616 (Batch  (None, 17, 17, 160)  480        ['conv2d_616[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_621 (Batch  (None, 17, 17, 160)  480        ['conv2d_621[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_616 (Activation)    (None, 17, 17, 160)  0           ['batch_normalization_616[0][0]']\n",
            "                                                                                                  \n",
            " activation_621 (Activation)    (None, 17, 17, 160)  0           ['batch_normalization_621[0][0]']\n",
            "                                                                                                  \n",
            " average_pooling2d_59 (AverageP  (None, 17, 17, 768)  0          ['mixed5[0][0]']                 \n",
            " ooling2D)                                                                                        \n",
            "                                                                                                  \n",
            " conv2d_614 (Conv2D)            (None, 17, 17, 192)  147456      ['mixed5[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_617 (Conv2D)            (None, 17, 17, 192)  215040      ['activation_616[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_622 (Conv2D)            (None, 17, 17, 192)  215040      ['activation_621[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_623 (Conv2D)            (None, 17, 17, 192)  147456      ['average_pooling2d_59[0][0]']   \n",
            "                                                                                                  \n",
            " batch_normalization_614 (Batch  (None, 17, 17, 192)  576        ['conv2d_614[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_617 (Batch  (None, 17, 17, 192)  576        ['conv2d_617[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_622 (Batch  (None, 17, 17, 192)  576        ['conv2d_622[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_623 (Batch  (None, 17, 17, 192)  576        ['conv2d_623[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_614 (Activation)    (None, 17, 17, 192)  0           ['batch_normalization_614[0][0]']\n",
            "                                                                                                  \n",
            " activation_617 (Activation)    (None, 17, 17, 192)  0           ['batch_normalization_617[0][0]']\n",
            "                                                                                                  \n",
            " activation_622 (Activation)    (None, 17, 17, 192)  0           ['batch_normalization_622[0][0]']\n",
            "                                                                                                  \n",
            " activation_623 (Activation)    (None, 17, 17, 192)  0           ['batch_normalization_623[0][0]']\n",
            "                                                                                                  \n",
            " mixed6 (Concatenate)           (None, 17, 17, 768)  0           ['activation_614[0][0]',         \n",
            "                                                                  'activation_617[0][0]',         \n",
            "                                                                  'activation_622[0][0]',         \n",
            "                                                                  'activation_623[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_628 (Conv2D)            (None, 17, 17, 192)  147456      ['mixed6[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_628 (Batch  (None, 17, 17, 192)  576        ['conv2d_628[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_628 (Activation)    (None, 17, 17, 192)  0           ['batch_normalization_628[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_629 (Conv2D)            (None, 17, 17, 192)  258048      ['activation_628[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_629 (Batch  (None, 17, 17, 192)  576        ['conv2d_629[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_629 (Activation)    (None, 17, 17, 192)  0           ['batch_normalization_629[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_625 (Conv2D)            (None, 17, 17, 192)  147456      ['mixed6[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_630 (Conv2D)            (None, 17, 17, 192)  258048      ['activation_629[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_625 (Batch  (None, 17, 17, 192)  576        ['conv2d_625[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_630 (Batch  (None, 17, 17, 192)  576        ['conv2d_630[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_625 (Activation)    (None, 17, 17, 192)  0           ['batch_normalization_625[0][0]']\n",
            "                                                                                                  \n",
            " activation_630 (Activation)    (None, 17, 17, 192)  0           ['batch_normalization_630[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_626 (Conv2D)            (None, 17, 17, 192)  258048      ['activation_625[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_631 (Conv2D)            (None, 17, 17, 192)  258048      ['activation_630[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_626 (Batch  (None, 17, 17, 192)  576        ['conv2d_626[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_631 (Batch  (None, 17, 17, 192)  576        ['conv2d_631[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_626 (Activation)    (None, 17, 17, 192)  0           ['batch_normalization_626[0][0]']\n",
            "                                                                                                  \n",
            " activation_631 (Activation)    (None, 17, 17, 192)  0           ['batch_normalization_631[0][0]']\n",
            "                                                                                                  \n",
            " average_pooling2d_60 (AverageP  (None, 17, 17, 768)  0          ['mixed6[0][0]']                 \n",
            " ooling2D)                                                                                        \n",
            "                                                                                                  \n",
            " conv2d_624 (Conv2D)            (None, 17, 17, 192)  147456      ['mixed6[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_627 (Conv2D)            (None, 17, 17, 192)  258048      ['activation_626[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_632 (Conv2D)            (None, 17, 17, 192)  258048      ['activation_631[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_633 (Conv2D)            (None, 17, 17, 192)  147456      ['average_pooling2d_60[0][0]']   \n",
            "                                                                                                  \n",
            " batch_normalization_624 (Batch  (None, 17, 17, 192)  576        ['conv2d_624[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_627 (Batch  (None, 17, 17, 192)  576        ['conv2d_627[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_632 (Batch  (None, 17, 17, 192)  576        ['conv2d_632[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_633 (Batch  (None, 17, 17, 192)  576        ['conv2d_633[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_624 (Activation)    (None, 17, 17, 192)  0           ['batch_normalization_624[0][0]']\n",
            "                                                                                                  \n",
            " activation_627 (Activation)    (None, 17, 17, 192)  0           ['batch_normalization_627[0][0]']\n",
            "                                                                                                  \n",
            " activation_632 (Activation)    (None, 17, 17, 192)  0           ['batch_normalization_632[0][0]']\n",
            "                                                                                                  \n",
            " activation_633 (Activation)    (None, 17, 17, 192)  0           ['batch_normalization_633[0][0]']\n",
            "                                                                                                  \n",
            " mixed7 (Concatenate)           (None, 17, 17, 768)  0           ['activation_624[0][0]',         \n",
            "                                                                  'activation_627[0][0]',         \n",
            "                                                                  'activation_632[0][0]',         \n",
            "                                                                  'activation_633[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_636 (Conv2D)            (None, 17, 17, 192)  147456      ['mixed7[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_636 (Batch  (None, 17, 17, 192)  576        ['conv2d_636[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_636 (Activation)    (None, 17, 17, 192)  0           ['batch_normalization_636[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_637 (Conv2D)            (None, 17, 17, 192)  258048      ['activation_636[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_637 (Batch  (None, 17, 17, 192)  576        ['conv2d_637[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_637 (Activation)    (None, 17, 17, 192)  0           ['batch_normalization_637[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_634 (Conv2D)            (None, 17, 17, 192)  147456      ['mixed7[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_638 (Conv2D)            (None, 17, 17, 192)  258048      ['activation_637[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_634 (Batch  (None, 17, 17, 192)  576        ['conv2d_634[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_638 (Batch  (None, 17, 17, 192)  576        ['conv2d_638[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_634 (Activation)    (None, 17, 17, 192)  0           ['batch_normalization_634[0][0]']\n",
            "                                                                                                  \n",
            " activation_638 (Activation)    (None, 17, 17, 192)  0           ['batch_normalization_638[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_635 (Conv2D)            (None, 8, 8, 320)    552960      ['activation_634[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_639 (Conv2D)            (None, 8, 8, 192)    331776      ['activation_638[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_635 (Batch  (None, 8, 8, 320)   960         ['conv2d_635[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_639 (Batch  (None, 8, 8, 192)   576         ['conv2d_639[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_635 (Activation)    (None, 8, 8, 320)    0           ['batch_normalization_635[0][0]']\n",
            "                                                                                                  \n",
            " activation_639 (Activation)    (None, 8, 8, 192)    0           ['batch_normalization_639[0][0]']\n",
            "                                                                                                  \n",
            " max_pooling2d_27 (MaxPooling2D  (None, 8, 8, 768)   0           ['mixed7[0][0]']                 \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " mixed8 (Concatenate)           (None, 8, 8, 1280)   0           ['activation_635[0][0]',         \n",
            "                                                                  'activation_639[0][0]',         \n",
            "                                                                  'max_pooling2d_27[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_644 (Conv2D)            (None, 8, 8, 448)    573440      ['mixed8[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_644 (Batch  (None, 8, 8, 448)   1344        ['conv2d_644[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_644 (Activation)    (None, 8, 8, 448)    0           ['batch_normalization_644[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_641 (Conv2D)            (None, 8, 8, 384)    491520      ['mixed8[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_645 (Conv2D)            (None, 8, 8, 384)    1548288     ['activation_644[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_641 (Batch  (None, 8, 8, 384)   1152        ['conv2d_641[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_645 (Batch  (None, 8, 8, 384)   1152        ['conv2d_645[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_641 (Activation)    (None, 8, 8, 384)    0           ['batch_normalization_641[0][0]']\n",
            "                                                                                                  \n",
            " activation_645 (Activation)    (None, 8, 8, 384)    0           ['batch_normalization_645[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_642 (Conv2D)            (None, 8, 8, 384)    442368      ['activation_641[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_643 (Conv2D)            (None, 8, 8, 384)    442368      ['activation_641[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_646 (Conv2D)            (None, 8, 8, 384)    442368      ['activation_645[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_647 (Conv2D)            (None, 8, 8, 384)    442368      ['activation_645[0][0]']         \n",
            "                                                                                                  \n",
            " average_pooling2d_61 (AverageP  (None, 8, 8, 1280)  0           ['mixed8[0][0]']                 \n",
            " ooling2D)                                                                                        \n",
            "                                                                                                  \n",
            " conv2d_640 (Conv2D)            (None, 8, 8, 320)    409600      ['mixed8[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_642 (Batch  (None, 8, 8, 384)   1152        ['conv2d_642[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_643 (Batch  (None, 8, 8, 384)   1152        ['conv2d_643[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_646 (Batch  (None, 8, 8, 384)   1152        ['conv2d_646[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_647 (Batch  (None, 8, 8, 384)   1152        ['conv2d_647[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_648 (Conv2D)            (None, 8, 8, 192)    245760      ['average_pooling2d_61[0][0]']   \n",
            "                                                                                                  \n",
            " batch_normalization_640 (Batch  (None, 8, 8, 320)   960         ['conv2d_640[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_642 (Activation)    (None, 8, 8, 384)    0           ['batch_normalization_642[0][0]']\n",
            "                                                                                                  \n",
            " activation_643 (Activation)    (None, 8, 8, 384)    0           ['batch_normalization_643[0][0]']\n",
            "                                                                                                  \n",
            " activation_646 (Activation)    (None, 8, 8, 384)    0           ['batch_normalization_646[0][0]']\n",
            "                                                                                                  \n",
            " activation_647 (Activation)    (None, 8, 8, 384)    0           ['batch_normalization_647[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_648 (Batch  (None, 8, 8, 192)   576         ['conv2d_648[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_640 (Activation)    (None, 8, 8, 320)    0           ['batch_normalization_640[0][0]']\n",
            "                                                                                                  \n",
            " mixed9_0 (Concatenate)         (None, 8, 8, 768)    0           ['activation_642[0][0]',         \n",
            "                                                                  'activation_643[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_12 (Concatenate)   (None, 8, 8, 768)    0           ['activation_646[0][0]',         \n",
            "                                                                  'activation_647[0][0]']         \n",
            "                                                                                                  \n",
            " activation_648 (Activation)    (None, 8, 8, 192)    0           ['batch_normalization_648[0][0]']\n",
            "                                                                                                  \n",
            " mixed9 (Concatenate)           (None, 8, 8, 2048)   0           ['activation_640[0][0]',         \n",
            "                                                                  'mixed9_0[0][0]',               \n",
            "                                                                  'concatenate_12[0][0]',         \n",
            "                                                                  'activation_648[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_653 (Conv2D)            (None, 8, 8, 448)    917504      ['mixed9[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_653 (Batch  (None, 8, 8, 448)   1344        ['conv2d_653[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_653 (Activation)    (None, 8, 8, 448)    0           ['batch_normalization_653[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_650 (Conv2D)            (None, 8, 8, 384)    786432      ['mixed9[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_654 (Conv2D)            (None, 8, 8, 384)    1548288     ['activation_653[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_650 (Batch  (None, 8, 8, 384)   1152        ['conv2d_650[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_654 (Batch  (None, 8, 8, 384)   1152        ['conv2d_654[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_650 (Activation)    (None, 8, 8, 384)    0           ['batch_normalization_650[0][0]']\n",
            "                                                                                                  \n",
            " activation_654 (Activation)    (None, 8, 8, 384)    0           ['batch_normalization_654[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_651 (Conv2D)            (None, 8, 8, 384)    442368      ['activation_650[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_652 (Conv2D)            (None, 8, 8, 384)    442368      ['activation_650[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_655 (Conv2D)            (None, 8, 8, 384)    442368      ['activation_654[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_656 (Conv2D)            (None, 8, 8, 384)    442368      ['activation_654[0][0]']         \n",
            "                                                                                                  \n",
            " average_pooling2d_62 (AverageP  (None, 8, 8, 2048)  0           ['mixed9[0][0]']                 \n",
            " ooling2D)                                                                                        \n",
            "                                                                                                  \n",
            " conv2d_649 (Conv2D)            (None, 8, 8, 320)    655360      ['mixed9[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_651 (Batch  (None, 8, 8, 384)   1152        ['conv2d_651[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_652 (Batch  (None, 8, 8, 384)   1152        ['conv2d_652[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_655 (Batch  (None, 8, 8, 384)   1152        ['conv2d_655[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_656 (Batch  (None, 8, 8, 384)   1152        ['conv2d_656[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_657 (Conv2D)            (None, 8, 8, 192)    393216      ['average_pooling2d_62[0][0]']   \n",
            "                                                                                                  \n",
            " batch_normalization_649 (Batch  (None, 8, 8, 320)   960         ['conv2d_649[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_651 (Activation)    (None, 8, 8, 384)    0           ['batch_normalization_651[0][0]']\n",
            "                                                                                                  \n",
            " activation_652 (Activation)    (None, 8, 8, 384)    0           ['batch_normalization_652[0][0]']\n",
            "                                                                                                  \n",
            " activation_655 (Activation)    (None, 8, 8, 384)    0           ['batch_normalization_655[0][0]']\n",
            "                                                                                                  \n",
            " activation_656 (Activation)    (None, 8, 8, 384)    0           ['batch_normalization_656[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_657 (Batch  (None, 8, 8, 192)   576         ['conv2d_657[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_649 (Activation)    (None, 8, 8, 320)    0           ['batch_normalization_649[0][0]']\n",
            "                                                                                                  \n",
            " mixed9_1 (Concatenate)         (None, 8, 8, 768)    0           ['activation_651[0][0]',         \n",
            "                                                                  'activation_652[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_13 (Concatenate)   (None, 8, 8, 768)    0           ['activation_655[0][0]',         \n",
            "                                                                  'activation_656[0][0]']         \n",
            "                                                                                                  \n",
            " activation_657 (Activation)    (None, 8, 8, 192)    0           ['batch_normalization_657[0][0]']\n",
            "                                                                                                  \n",
            " mixed10 (Concatenate)          (None, 8, 8, 2048)   0           ['activation_649[0][0]',         \n",
            "                                                                  'mixed9_1[0][0]',               \n",
            "                                                                  'concatenate_13[0][0]',         \n",
            "                                                                  'activation_657[0][0]']         \n",
            "                                                                                                  \n",
            " global_average_pooling2d_6 (Gl  (None, 2048)        0           ['mixed10[0][0]']                \n",
            " obalAveragePooling2D)                                                                            \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 6)            12294       ['global_average_pooling2d_6[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 21,815,078\n",
            "Trainable params: 12,294\n",
            "Non-trainable params: 21,802,784\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.layers[-1].activation = None\n",
        "img_path = \"shrink/shrink_testdata/shrink_testdata16.jpg\"\n",
        "img_array = gc.get_img_array(img_path)\n",
        "last_conv_layer_name = \"conv2d_657\"\n",
        "heatmap = gc.make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n",
        "gc.save_and_display_gradcam(img_path, heatmap, cam_path=\"cam6.jpg\")"
      ],
      "metadata": {
        "id": "0cspW796sCRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_shrink = pd.DataFrame()\n",
        "df_shrink[\"Predict_Class\"] = pred_label_name\n",
        "df_shrink[\"True_Class\"] = correct_label_name\n",
        "print(pd.crosstab(df_shrink[\"Predict_Class\"], df_shrink[\"True_Class\"]))\n",
        "print(\"サンプル数：\", len(df_shrink[\"Predict_Class\"]))\n",
        "print(\"正解数：\", sum(df_shrink[\"Predict_Class\"] == df_shrink[\"True_Class\"]))"
      ],
      "metadata": {
        "id": "mls--vSSW1t6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.layers[-1].activation = None\n",
        "img_path = \"shrink/shrink_testdata/shrink_testdata16.jpg\"\n",
        "img_array = gc.get_img_array(img_path)\n",
        "last_conv_layer_name = \"conv2d_657\"\n",
        "heatmap = gc.make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n",
        "gc.save_and_display_gradcam(img_path, heatmap, cam_path=\"cam6.jpg\")"
      ],
      "metadata": {
        "id": "YPPErj54LjVX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}